{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"idal-logo.png\" align=\"right\" style=\"float\" width=\"400\">\n",
    "<font color=\"#CA3532\"><h1 align=\"left\">mIA3</h1></font>\n",
    "<font color=\"#6E6E6E\"><h2 align=\"left\">Tarea Evaluable. Aprendizaje Profundo 1 y 2 (Parte 2 de 3).</h2></font> \n",
    "\n",
    "#### Elaborado por Felipe Ramírez Herrera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install panda\n",
    "\n",
    "# !pip install torch\n",
    "# !pip install torchtext\n",
    "# !pip install torchmetrics\n",
    "# !pip install spacy\n",
    "\n",
    "# !pip install jupyter\n",
    "# !pip install ipywidgets\n",
    "\n",
    "# !pip install editdistance\n",
    "# !pip install six\n",
    "# !pip install typeguard\n",
    "\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn\n",
    "# !pip install scikit-learn\n",
    "\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download es_core_news_sm\n",
    "# !python -m spacy download fr_core_news_sm\n",
    "# !pip install wget\n",
    "\n",
    "# !pip install --upgrade --force-reinstall torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import csv\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import Vocab\n",
    "import logging\n",
    "import pandas as pd\n",
    "import six\n",
    "from typing import List, Tuple, Union\n",
    "from argparse import Namespace\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as mp\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formato de Mathplotlib\n",
    "\n",
    "mp.rc('font', size=8)\n",
    "mp.rc('axes', titlesize=8)\n",
    "mp.rc('axes', labelsize=8)\n",
    "mp.rc('xtick', labelsize=8)\n",
    "mp.rc('ytick', labelsize=8)\n",
    "mp.rc('legend', fontsize=8)\n",
    "mp.rc('figure', titlesize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cuda_memory_available():\n",
    "    if torch.cuda.is_available():\n",
    "        t = torch.cuda.get_device_properties(DEVICE).total_memory\n",
    "        r = torch.cuda.memory_reserved(DEVICE)\n",
    "        a = torch.cuda.memory_allocated(DEVICE)\n",
    "        f = (t - a - r) / t  # free inside reserved\n",
    "        print(\"CUDA Available Memory: {0}\".format(f))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model, return_int=False):\n",
    "    params = sum([torch.prod(torch.tensor(x.shape)).item() for x in model.parameters() if x.requires_grad])\n",
    "    if return_int:\n",
    "        return params\n",
    "    else:\n",
    "        print(\"There are {:,} trainable parameters in this model.\".format(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://conferences.unite.un.org/UNCorpus/Home/DownloadOverview\n",
    "# !cat /home/framirez/translation_multilingual/UNv1.0.6way.tar.gz.* | tar -xzf -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "\n",
    "# EN_corpus_file = \"UNv1.0.6way_en.zip\" # UN Parallel Corpus English (EN)\n",
    "# ES_corpus_file = \"UNv1.0.6way_es.zip\" # UN Parallel Corpus Spanish (ES)\n",
    "# FR_corpus_file = \"UNv1.0.6way_fr.zip\" # UN Parallel Corpus French (FR)\n",
    "\n",
    "# corpus_source_folder = \"/content/drive/MyDrive/\"\n",
    "# corpus_target_folder = \"/tmp/\"\n",
    "\n",
    "# if (not os.path.exists(corpus_target_folder + EN_corpus_file)):\n",
    "#   with zipfile.ZipFile(corpus_source_folder + EN_corpus_file,\"r\") as zip_ref:\n",
    "#       zip_ref.extractall(corpus_target_folder)\n",
    "\n",
    "# if (not os.path.exists(corpus_target_folder + ES_corpus_file)):\n",
    "#   with zipfile.ZipFile(corpus_source_folder + ES_corpus_file,\"r\") as zip_ref:\n",
    "#       zip_ref.extractall(corpus_target_folder)\n",
    "\n",
    "# if (not os.path.exists(corpus_target_folder + FR_corpus_file)):\n",
    "#   with zipfile.ZipFile(corpus_source_folder + FR_corpus_file,\"r\") as zip_ref:\n",
    "#       zip_ref.extractall(corpus_target_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the cuDNN backend for 16-bit training and inference for CUDA-enabled GPUs\n",
    "# torch.backends.cudnn.benchmark =  True\n",
    "# torch.backends.cudnn.enabled =  True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 64 # Re-calculated further \n",
    "min_seq_length = 16 # Re-calculated further "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUMBER_OF_EPOCHS = 2\n",
    "EARLY_STOPPING_EPOCHS = 3\n",
    "CLIPPING_VALUE = 15                   # clipping value, or None to prevent gradient clipping\n",
    "MAXIMUM_NUMBER_OF_SAMPLES = 500000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Lets GPU to get more cooler\n",
    "INNER_GPU_REST_TIME = 0 # Rest time between epochs\n",
    "OUTER_GPU_REST_TIME = 1 # Rest time between model training / validation processes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "\n",
    "def generate_examples(src_file, tgt_a_file, tgt_b_file):\n",
    "    data = []\n",
    "    with open(src_file, encoding=\"utf-8\") as src_f, open(tgt_a_file, encoding=\"utf-8\") as tgt_a, open(tgt_b_file, encoding=\"utf-8\") as tgt_b:\n",
    "        for idx, (a, b, c) in enumerate(zip(src_f, tgt_a, tgt_b)):           \n",
    "            if (a.isspace() | b.isspace() | c.isspace()):\n",
    "                continue\n",
    "            data.append({'text_en' : a, 'text_es' : b, 'text_fr' :c} )\n",
    "    return pd.DataFrame.from_records(data=data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from unicodedata import normalize\n",
    "\n",
    "PAD_WORD = '<pad>'\n",
    "UNK_WORD = '<unk>'  # Unknown token symbol\n",
    "BOS_WORD = '<bos>'  # Begin-of-Sentence token symbol\n",
    "EOS_WORD = '<eos>'  # End-of-Sentence token symbol\n",
    "\n",
    "def clean_text(text : str):\n",
    "    text = normalize('NFD', str(text).lower())\n",
    "    text = re.sub('[^A-Za-z ]+', '', text)\n",
    "    return text\n",
    "\n",
    "pkl_dataset_file = \"un_parallel_corpus.pkl\"\n",
    "un_ds = pd.DataFrame()\n",
    "\n",
    "if os.path.exists(pkl_dataset_file):\n",
    "    un_ds = pd.read_pickle(pkl_dataset_file) \n",
    "else:\n",
    "    en_train_path = \"./mnt/drive/UNv1.0.6way.en\"\n",
    "    es_train_path = \"./mnt/drive/UNv1.0.6way.es\"\n",
    "    fr_train_path = \"./mnt/drive/UNv1.0.6way.fr\"\n",
    "    df = generate_examples(en_train_path, es_train_path, fr_train_path)\n",
    "    df['text_en'] = df['text_en'].apply(lambda row: clean_text(row))\n",
    "    df['text_es'] = df['text_es'].apply(lambda row: clean_text(row))\n",
    "    df['text_fr'] = df['text_fr'].apply(lambda row: clean_text(row))\n",
    "    un_ds = df\n",
    "    un_ds.to_pickle(pkl_dataset_file)\n",
    "len(un_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_ds.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_ds.head(-25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_ds = un_ds.head(MAXIMUM_NUMBER_OF_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "import torchtext\n",
    "\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "es_tokenizer = get_tokenizer('spacy', language='es_core_news_sm')\n",
    "fr_tokenizer = get_tokenizer('spacy', language='fr_core_news_sm')\n",
    "\n",
    "Language = Enum('Language', ['EN', 'ES', 'FR'])\n",
    "\n",
    "def yield_tokens(Lang: Language=Language.EN):\n",
    "    for index, row in un_ds.iterrows():     \n",
    "         if (Lang == Language.EN):\n",
    "            yield en_tokenizer(str(row[\"text_en\"]))  \n",
    "         if (Lang == Language.ES):\n",
    "            yield es_tokenizer(str(row[\"text_es\"]))\n",
    "         if (Lang == Language.FR):\n",
    "           yield fr_tokenizer(str(row[\"text_fr\"]))\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab_file = 'transformers_vocab_en.pth'\n",
    "es_vocab_file = 'transformers_vocab_es.pth'\n",
    "fr_vocab_file = 'transformers_vocab_fr.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import torchtext.data\n",
    "import torchtext.data.datasets_utils\n",
    "import torchtext.datasets\n",
    "\n",
    "SPECIALS = [PAD_WORD, BOS_WORD, EOS_WORD,  UNK_WORD]\n",
    "\n",
    "en_vocabulary = None\n",
    "es_vocabulary = None\n",
    "fr_vocabulary = None\n",
    "\n",
    "if os.path.exists(en_vocab_file):\n",
    "    en_vocabulary = torch.load(en_vocab_file)\n",
    "else:\n",
    "    en_vocabulary = build_vocab_from_iterator(yield_tokens(Language.EN), specials=SPECIALS)\n",
    "    torch.save(en_vocabulary, en_vocab_file)\n",
    "\n",
    "if os.path.exists(es_vocab_file):\n",
    "    es_vocabulary = torch.load(es_vocab_file)\n",
    "else:\n",
    "    es_vocabulary = build_vocab_from_iterator(yield_tokens(Language.ES), specials=SPECIALS)\n",
    "    torch.save(es_vocabulary, es_vocab_file)\n",
    "\n",
    "if os.path.exists(fr_vocab_file):\n",
    "    fr_vocabulary = torch.load(fr_vocab_file)\n",
    "else:\n",
    "    fr_vocabulary = build_vocab_from_iterator(yield_tokens(Language.FR), specials=SPECIALS)\n",
    "    torch.save(fr_vocabulary, fr_vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EN Vocabulary Size = {0}\".format(len(en_vocabulary)))\n",
    "print(\"ES Vocabulary Size = {0}\".format(len(es_vocabulary)))\n",
    "print(\"FR Vocabulary Size = {0}\".format(len(fr_vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_EN_IDX = en_vocabulary[PAD_WORD]\n",
    "BOS_EN_IDX = en_vocabulary[BOS_WORD]\n",
    "EOS_EN_IDX = en_vocabulary[EOS_WORD]\n",
    "UNK_EN_IDX = en_vocabulary[UNK_WORD]\n",
    "\n",
    "\n",
    "PAD_ES_IDX = es_vocabulary[PAD_WORD]\n",
    "BOS_ES_IDX = es_vocabulary[BOS_WORD]\n",
    "EOS_ES_IDX = es_vocabulary[EOS_WORD]\n",
    "UNK_ES_IDX = es_vocabulary[UNK_WORD]\n",
    "\n",
    "PAD_FR_IDX = fr_vocabulary[PAD_WORD]\n",
    "BOS_FR_IDX = fr_vocabulary[BOS_WORD]\n",
    "EOS_FR_IDX = fr_vocabulary[EOS_WORD]\n",
    "UNK_FR_IDX = fr_vocabulary[UNK_WORD]\n",
    "\n",
    "en_vocab_size = len(en_vocabulary) + 1\n",
    "es_vocab_size = len(es_vocabulary) + 1\n",
    "fr_vocab_size = len(fr_vocabulary) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate(tokenized_text, allocate : bool = True,  pad_index : int = PAD_EN_IDX, bos_index : int = BOS_EN_IDX, eos_index : int = EOS_EN_IDX):  \n",
    "    result = []\n",
    "    if len(tokenized_text) < max_seq_length:\n",
    "        if (allocate):\n",
    "            result = [bos_index] + tokenized_text + [eos_index]\n",
    "            left = max_seq_length - len(result)\n",
    "            padding = [pad_index] * left\n",
    "            result = result + padding\n",
    "        else:\n",
    "            left = max_seq_length - len(tokenized_text)\n",
    "            padding = [pad_index] * left\n",
    "            result = tokenized_text + padding       \n",
    "    else:\n",
    "        raise Exception(\"pad_or_truncate: max_seq_length not computed properly\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_file = 'transformers_full.pth'\n",
    "trn_data_file = 'transformers_trn.pth'\n",
    "val_data_file = 'transformers_val.pth'\n",
    "tst_data_file = 'transformers_tst.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(un_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "from collections import Counter \n",
    "\n",
    "MINIMUM_ALLOWED_SIZE_OF_SEQ = 5\n",
    "MAXIMUM_ALLOWED_SIZE_OF_SEQ = 100\n",
    "\n",
    "full_data = []\n",
    "trn_subset, val_subset, tst_subset = [], [], []\n",
    "random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "max_len = 0\n",
    "min_len = 4500\n",
    "\n",
    "en_counter = Counter()\n",
    "es_counter = Counter()\n",
    "fr_counter = Counter()\n",
    "\n",
    "en_lengths = []\n",
    "es_lengths = []\n",
    "fr_lengths = []\n",
    "\n",
    "fe = os.path.exists(full_data_file)\n",
    "te = os.path.exists(trn_data_file)\n",
    "ve = os.path.exists(val_data_file)\n",
    "tt = os.path.exists(tst_data_file)\n",
    "\n",
    "if fe and te and ve and tt:\n",
    "    with open(full_data_file, 'rb') as fp:\n",
    "            full_data = pickle.load(fp)\n",
    "    with open(trn_data_file, 'rb') as fp:\n",
    "            trn_subset = pickle.load(fp)\n",
    "    with open(val_data_file, 'rb') as fp:\n",
    "            val_subset = pickle.load(fp)        \n",
    "    with open(tst_data_file, 'rb') as fp:\n",
    "            tst_subset = pickle.load(fp)           \n",
    "    for (en_seq,es_seq,fr_seq) in full_data:     \n",
    "        en_counter.update(en_seq)\n",
    "        es_counter.update(es_seq)\n",
    "        fr_counter.update(fr_seq)\n",
    "        a = len(en_seq)\n",
    "        b = len(es_seq)\n",
    "        c = len(fr_seq) \n",
    "        en_lengths.append(a)\n",
    "        es_lengths.append(b)\n",
    "        fr_lengths.append(c)\n",
    "        max_len = max(max_len, a, b, c)    \n",
    "        min_len = min(min_len, a, b, c)    \n",
    "else:\n",
    "    for idx, row in un_ds.iterrows():\n",
    "        \n",
    "        en_exp = row[\"text_en\"].rstrip(\"\\n\")\n",
    "        es_exp = row[\"text_es\"].rstrip(\"\\n\")\n",
    "        fr_exp = row[\"text_fr\"].rstrip(\"\\n\")\n",
    "\n",
    "        en_seq = [en_vocabulary[token] for token in en_tokenizer(en_exp)]\n",
    "        es_seq = [es_vocabulary[token] for token in es_tokenizer(es_exp)]\n",
    "        fr_seq = [fr_vocabulary[token] for token in fr_tokenizer(fr_exp)]\n",
    "\n",
    "        a = len(en_seq)\n",
    "        b = len(es_seq)\n",
    "        c = len(fr_seq) \n",
    "\n",
    "        seq_min = min(a, b, c)\n",
    "        seq_max = max(a, b, c)\n",
    "\n",
    "        if seq_min  >= MINIMUM_ALLOWED_SIZE_OF_SEQ and seq_max <= MAXIMUM_ALLOWED_SIZE_OF_SEQ: \n",
    "            \n",
    "            en_counter.update(en_seq)\n",
    "            es_counter.update(es_seq)\n",
    "            fr_counter.update(fr_seq)\n",
    "       \n",
    "            en_lengths.append(a)\n",
    "            es_lengths.append(b)\n",
    "            fr_lengths.append(c)\n",
    "\n",
    "            max_len = max(max_len, seq_max)\n",
    "            min_len = min(min_len, seq_min) \n",
    "            full_data.append((en_seq, es_seq, fr_seq))\n",
    "\n",
    "    trn_subset, val_subset = train_test_split(full_data, test_size=0.3, train_size=0.7, random_state=1234, shuffle=True)\n",
    "    val_subset, tst_subset = train_test_split(val_subset, test_size=0.33, train_size=0.67, random_state=1234, shuffle=True)\n",
    "\n",
    "    with open(full_data_file, 'wb') as fp:\n",
    "        pickle.dump(full_data, fp)\n",
    "    with open(trn_data_file, 'wb') as fp:\n",
    "        pickle.dump(trn_subset, fp)\n",
    "    with open(val_data_file, 'wb') as fp:\n",
    "        pickle.dump(val_subset, fp)\n",
    "    with open(tst_data_file, 'wb') as fp:\n",
    "        pickle.dump(tst_subset, fp)\n",
    "\n",
    "if (max_seq_length < max_len):\n",
    "    max_seq_length = max_len + 2\n",
    "\n",
    "if (min_seq_length > min_len):\n",
    "    min_seq_length = min_len + 2\n",
    "\n",
    "\n",
    "print(\"Selected Records: {0}\".format(len(full_data)))\n",
    "print(\"MAX SEQ LEN {0}\".format(max_seq_length))\n",
    "print(\"MIN SEQ LEN {0}\".format(min_seq_length))\n",
    "\n",
    "size_of_trn_set_size = len(trn_subset)\n",
    "size_of_val_set_size = len(val_subset)\n",
    "size_of_tst_set_size = len(tst_subset)\n",
    "\n",
    "print(\"Training tuples: {0} Validation tuples: {1} Testing tuples: {2}\".format(size_of_trn_set_size, size_of_val_set_size, size_of_tst_set_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = mp.figure(figsize=(8, 10))\n",
    "ax0 = fig.add_subplot(311)\n",
    "ax0.hist(en_lengths, rwidth=0.8, color='gray')\n",
    "ax0.set_title(\"English Sentence Length\")\n",
    "ax0.set_xlabel(\"# Tokens in Sentence\")\n",
    "\n",
    "ax1 = fig.add_subplot(312)\n",
    "ax1.hist(es_lengths, rwidth=0.8, color='gray')\n",
    "ax1.set_title(\"Spanish Sentence Length\")\n",
    "ax1.set_xlabel(\"# Tokens in Sentence\")\n",
    "\n",
    "ax2 = fig.add_subplot(313)\n",
    "ax2.hist(fr_lengths, rwidth=0.8, color='gray')\n",
    "ax2.set_title(\"French Sentence Length\")\n",
    "ax2.set_xlabel(\"# Tokens in Sentence\")\n",
    "\n",
    "mp.tight_layout()\n",
    "mp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.figure(figsize=(8,6))\n",
    "mp.hist2d(en_lengths, es_lengths, bins=max_seq_length-2, cmap='binary')\n",
    "mp.title(\"Joint Distribution of Sentence Lengths\")\n",
    "mp.xlabel(\"# English Tokens\")\n",
    "mp.ylabel(\"# Spanish Tokens\")\n",
    "mp.colorbar()\n",
    "mp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.figure(figsize=(8,6))\n",
    "mp.hist2d(en_lengths, fr_lengths, bins=max_seq_length-2, cmap='binary')\n",
    "mp.title(\"Joint Distribution of Sentence Lengths\")\n",
    "mp.xlabel(\"# English Tokens\")\n",
    "mp.ylabel(\"# French Tokens\")\n",
    "mp.colorbar()\n",
    "mp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(counter : Counter, vocab: Vocab, k=20, ax=None):\n",
    "    top_k = counter.most_common(k)\n",
    "    tokens, freqs = zip(*reversed(top_k))\n",
    "    \n",
    "    words = [vocab.lookup_token(token) for token in tokens]\n",
    "\n",
    "    if ax is None:\n",
    "        mp.barh(words, freqs, color='gray')\n",
    "    else:\n",
    "        ax.barh(words, freqs, color='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = mp.figure(figsize=(14, 8))\n",
    "\n",
    "ax0 = fig.add_subplot(131)\n",
    "plot_top_words(en_counter, en_vocabulary, ax=ax0)\n",
    "ax0.set_title(\"Top 20 English Words\")\n",
    "ax0.set_xlabel(\"Raw Frequency\")\n",
    "\n",
    "ax1 = fig.add_subplot(132)\n",
    "plot_top_words(es_counter,es_vocabulary, ax=ax1)\n",
    "ax1.set_title(\"Top 20 Spanish Words\")\n",
    "ax1.set_xlabel(\"Raw Frequency\")\n",
    "\n",
    "ax2 = fig.add_subplot(133)\n",
    "plot_top_words(fr_counter,fr_vocabulary, ax=ax2)\n",
    "ax2.set_title(\"Top 20 French Words\")\n",
    "ax2.set_xlabel(\"Raw Frequency\")\n",
    "\n",
    "mp.tight_layout()\n",
    "mp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "for en_id, es_id, fr_id in zip(en_vocabulary.lookup_indices(SPECIALS), es_vocabulary.lookup_indices(SPECIALS), fr_vocabulary.lookup_indices(SPECIALS)):\n",
    "  assert en_id == es_id & es_id == fr_id\n",
    "\n",
    "\n",
    "def tensor_transform(token_ids: List[int], bos_idx, eos_idx): # 57.1 segs\n",
    "    list = [bos_idx] + token_ids + [eos_idx]                         \n",
    "    return torch.as_tensor(list, device=DEVICE)\n",
    "\n",
    "# def tensor_transform(token_ids: List[int], bos_idx, eos_idx):\n",
    "#     return torch.cat((torch.tensor([bos_idx], device=DEVICE),\n",
    "#                       torch.tensor(token_ids, device=DEVICE),\n",
    "#                       torch.tensor([eos_idx], device=DEVICE)))\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "    en_batch, es_batch, fr_batch = [], [], []\n",
    "    for (en_item, es_item, fr_item) in data_batch:     \n",
    "        en_t = tensor_transform(en_item, BOS_EN_IDX, EOS_EN_IDX)\n",
    "        es_t = tensor_transform(es_item, BOS_ES_IDX, EOS_ES_IDX)\n",
    "        fr_t = tensor_transform(fr_item, BOS_FR_IDX, EOS_FR_IDX)     \n",
    "        en_batch.append(en_t)\n",
    "        es_batch.append(es_t)\n",
    "        fr_batch.append(fr_t)\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_EN_IDX, batch_first=False)\n",
    "    es_batch = pad_sequence(es_batch, padding_value=PAD_ES_IDX, batch_first=False)\n",
    "    fr_batch = pad_sequence(fr_batch, padding_value=PAD_FR_IDX, batch_first=False)\n",
    "    return en_batch, es_batch, fr_batch\n",
    "\n",
    "def generate_batch_EN_ES(data_batch):\n",
    "    en_batch, es_batch = [], []\n",
    "    for (en_item, es_item, _) in data_batch:     \n",
    "        en_t = tensor_transform(en_item, BOS_EN_IDX, EOS_EN_IDX)\n",
    "        es_t = tensor_transform(es_item, BOS_ES_IDX, EOS_ES_IDX)\n",
    "        en_batch.append(en_t)\n",
    "        es_batch.append(es_t)\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_EN_IDX, batch_first=False)\n",
    "    es_batch = pad_sequence(es_batch, padding_value=PAD_ES_IDX, batch_first=False)\n",
    "    return en_batch, es_batch\n",
    "\n",
    "def generate_batch_EN_FR(data_batch):\n",
    "    en_batch, fr_batch = [], []\n",
    "    for (en_item, _, fr_item) in data_batch:        \n",
    "        en_t = tensor_transform(en_item, BOS_EN_IDX, EOS_EN_IDX)\n",
    "        fr_t = tensor_transform(fr_item, BOS_FR_IDX, EOS_FR_IDX)\n",
    "        en_batch.append(en_t)\n",
    "        fr_batch.append(fr_t)\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_EN_IDX, batch_first=False)\n",
    "    fr_batch = pad_sequence(fr_batch, padding_value=PAD_ES_IDX, batch_first=False)\n",
    "    return en_batch, fr_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class LanguageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, subset, length):\n",
    "        self.length = length\n",
    "        self.subset = subset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.subset[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Bilingual_EN_to_ES_trnset = DataLoader(LanguageDataset(trn_subset, size_of_trn_set_size), batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch_EN_ES)\n",
    "Bilingual_EN_to_ES_valset = DataLoader(LanguageDataset(val_subset, size_of_val_set_size), batch_size=BATCH_SIZE, shuffle=False, collate_fn=generate_batch_EN_ES)\n",
    "Bilingual_EN_to_ES_tstset = DataLoader(LanguageDataset(tst_subset, size_of_tst_set_size), batch_size=BATCH_SIZE, shuffle=False, collate_fn=generate_batch_EN_ES)\n",
    "\n",
    "Bilingual_EN_to_FR_trnset = DataLoader(LanguageDataset(trn_subset, size_of_trn_set_size), batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch_EN_FR)\n",
    "Bilingual_EN_to_FR_valset = DataLoader(LanguageDataset(val_subset, size_of_val_set_size), batch_size=BATCH_SIZE, shuffle=False, collate_fn=generate_batch_EN_FR)\n",
    "Bilingual_EN_to_FR_tstset = DataLoader(LanguageDataset(tst_subset, size_of_tst_set_size), batch_size=BATCH_SIZE, shuffle=False, collate_fn=generate_batch_EN_FR)\n",
    "\n",
    "Trilingual_trnset = DataLoader(LanguageDataset(trn_subset, size_of_trn_set_size), batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
    "Trilingual_valset = DataLoader(LanguageDataset(val_subset, size_of_val_set_size), batch_size=BATCH_SIZE, shuffle=False, collate_fn=generate_batch)\n",
    "Trilingual_tstset = DataLoader(LanguageDataset(tst_subset, size_of_tst_set_size), batch_size=BATCH_SIZE, shuffle=False, collate_fn=generate_batch)\n",
    "\n",
    "batches_for_training = math.ceil( size_of_trn_set_size / BATCH_SIZE)\n",
    "batches_for_validation = math.ceil( size_of_val_set_size / BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "size_to_MB = 1024 * 1024\n",
    "\n",
    "def getSizeOf(a : torch.Tensor):\n",
    "    return sys.getsizeof(a) + torch.numel(a) * a.element_size()\n",
    "\n",
    "def ElementsOf(a : torch.Tensor):\n",
    "    return torch.numel(a)\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "total = 0\n",
    "\n",
    "for (src, tgt_ES, tgt_FR) in iter(Trilingual_trnset):\n",
    "\n",
    "    total += getSizeOf(src) / size_to_MB\n",
    "    total += getSizeOf(tgt_ES) / size_to_MB\n",
    "    total += getSizeOf(tgt_FR) / size_to_MB\n",
    "\n",
    "    del src\n",
    "    del tgt_ES\n",
    "    del tgt_FR\n",
    "\n",
    "print(\"Tamaño estimado del conjunto de datos: {} Mbytes\".format(total))\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcionalidad compartida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class GarbageCollector:\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.collected = False\n",
    "        self.registry = []\n",
    "\n",
    "    def __enter__(self):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.collected = True\n",
    "\n",
    "        for T in self.registry:\n",
    "            del T\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def Register(self, T : torch.Tensor):\n",
    "        if (not self.collected):\n",
    "            self.registry.append(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fragmento de código. Funciones que despliegan las curvas de desempeño para el modelo\n",
    "# Se reutilizan a lo largo del ejercicio.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "PLOT_X = 10\n",
    "PLOT_Y = 2.7    \n",
    "\n",
    "def plot_accuracy_curve(name, history):\n",
    "    fig, ax = plt.subplots(figsize=(PLOT_X, PLOT_Y), layout='constrained')\n",
    "    ax.set_title('Exactitud del modelo {0}'.format(name))\n",
    "    ax.plot(history['train_acc'], label='Entrenamiento', linestyle='solid', lw=2.5)\n",
    "    ax.plot(history['valid_acc'], label='Validación', linestyle='dashed', lw=2.5)\n",
    "    ax.set_xticks(np.arange(1, NUMBER_OF_EPOCHS + 1, 1))\n",
    "    ax.set_yticks(np.arange(0, 1, 1 / 10))\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Exactitud')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss_curve(name, history):\n",
    "    fig, ax = plt.subplots(figsize=(PLOT_X, PLOT_Y), layout='constrained')\n",
    "    ax.set_title('Pérdida del modelo {0}'.format(name))\n",
    "    ax.plot(history['train_loss'], label='Entrenamiento', linestyle='solid', lw=2.5)\n",
    "    ax.plot(history['valid_loss'], label='Validación', linestyle='dashed', lw=2.5)\n",
    "    ax.set_xticks(np.arange(1, NUMBER_OF_EPOCHS + 1, 1))\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Pérdida')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_pplx_curve(name, history):\n",
    "    fig, ax = plt.subplots(figsize=(PLOT_X, PLOT_Y), layout='constrained')\n",
    "    ax.set_title('Perplejidad del modelo {0}'.format(name))\n",
    "    ax.plot(history['train_ppl'], label='Entrenamiento', linestyle='solid', lw=2.5)\n",
    "    ax.plot(history['valid_ppl'], label='Validación', linestyle='dashed', lw=2.5)\n",
    "    ax.set_xticks(np.arange(1, NUMBER_OF_EPOCHS + 1, 1))\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Perplejidad')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_accuracy_curve_dual_transformer(name, history):\n",
    "    fig, ax = plt.subplots(figsize=(PLOT_X, PLOT_Y), layout='constrained')\n",
    "    ax.set_title('Exactitud del modelo {0}'.format(name))\n",
    "    ax.plot(history['train_joint_accm'], label='Entrenamiento', linestyle='solid', lw=2.5)\n",
    "    ax.plot(history['valid_joint_accm'], label='Validación', linestyle='dashed', lw=2.5)\n",
    "    ax.set_xticks(np.arange(1, NUMBER_OF_EPOCHS + 1, 1))\n",
    "    ax.set_yticks(np.arange(0, 1, 1 / 10))\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Exactitud')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_loss_curve_dual_transformer(name, history):\n",
    "    fig, ax = plt.subplots(figsize=(PLOT_X, PLOT_Y), layout='constrained')\n",
    "    ax.set_title('Pérdida del modelo {0}'.format(name))\n",
    "    ax.plot(history['train_joint_loss'], label='Entrenamiento', linestyle='solid', lw=2.5)\n",
    "    ax.plot(history['valid_joint_loss'], label='Validación', linestyle='dashed', lw=2.5)\n",
    "    ax.set_xticks(np.arange(1, NUMBER_OF_EPOCHS + 1, 1))\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Pérdida')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy_curve_dual_transformer_both(name, history):\n",
    "    fig, ax = plt.subplots(figsize=(PLOT_X, PLOT_Y), layout='constrained')\n",
    "    ax.set_title('Exactitud del modelo {0}'.format(name))\n",
    "    ax.plot(history['train_acc_a'], label='Entrenamiento (EN-ES)')\n",
    "    ax.plot(history['train_acc_b'], label='Entrenamiento (EN-FR)')\n",
    "    ax.plot(history['valid_acc_a'], label='Validación (EN-ES)')\n",
    "    ax.plot(history['valid_acc_b'], label='Validación (EN-FR)')\n",
    "    ax.set_xticks(np.arange(1, NUMBER_OF_EPOCHS + 1, 1))\n",
    "    ax.set_yticks(np.arange(0, 1, 1 / 10))\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Exactitud')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss_curve_dual_transformer_both(name, history):\n",
    "    fig, ax = plt.subplots(figsize=(PLOT_X, PLOT_Y), layout='constrained')\n",
    "    ax.set_title('Pérdida del modelo {0}'.format(name))\n",
    "    ax.plot(history['train_loss_a'], label='Entrenamiento (EN-ES)')\n",
    "    ax.plot(history['train_loss_b'], label='Entrenamiento (EN-FR)')\n",
    "    ax.plot(history['valid_loss_a'], label='Validación (EN-ES)')\n",
    "    ax.plot(history['valid_loss_b'], label='Validación (EN-FR)')\n",
    "    ax.set_xticks(np.arange(1, NUMBER_OF_EPOCHS + 1, 1))\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Pérdida')\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def let_gpu_rest(minutes):\n",
    "    if minutes > 0:\n",
    "        time.sleep(minutes * 60)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_data_sizing():\n",
    "    size_of_epoch = 0\n",
    "    batches = 0\n",
    "    elements = 0\n",
    "    for (src, tgt_ES, tgt_FR) in iter(Trilingual_trnset):\n",
    "        tgt_a_input = tgt_ES[:-1, :]\n",
    "        tgt_b_input = tgt_FR[:-1, :]\n",
    "        src_mask, tgt_a_mask, tgt_b_mask, src_padding_mask, tgt_a_padding_mask, tgt_b_padding_mask = create_mask_for_dual_decoder(src, tgt_a_input, tgt_b_input)\n",
    "        elements += ElementsOf(src)\n",
    "        elements += ElementsOf(tgt_ES)\n",
    "        elements += ElementsOf(tgt_FR)\n",
    "        size_of_epoch += getSizeOf(src)\n",
    "        size_of_epoch += getSizeOf(tgt_ES)\n",
    "        size_of_epoch += getSizeOf(tgt_FR)\n",
    "        size_of_epoch += getSizeOf(tgt_a_input)\n",
    "        size_of_epoch += getSizeOf(tgt_b_input)\n",
    "        size_of_epoch += getSizeOf(src_mask)\n",
    "        size_of_epoch += getSizeOf(tgt_a_mask)\n",
    "        size_of_epoch += getSizeOf(tgt_b_mask)\n",
    "        size_of_epoch += getSizeOf(src_padding_mask)\n",
    "        size_of_epoch += getSizeOf(tgt_a_padding_mask)\n",
    "        size_of_epoch += getSizeOf(tgt_b_padding_mask)\n",
    "        del src\n",
    "        del tgt_ES\n",
    "        del tgt_FR\n",
    "        del tgt_a_input\n",
    "        del tgt_b_input\n",
    "        del src_mask\n",
    "        del tgt_a_mask\n",
    "        del tgt_b_mask\n",
    "        del src_padding_mask\n",
    "        del tgt_a_padding_mask\n",
    "        del tgt_b_padding_mask\n",
    "        batches += 1\n",
    "        \n",
    "    print(\"Tamaño estimado del conjunto de datos: {0} Mbytes en {1} batches = {2} elements\".format(size_of_epoch / size_to_MB, batches, elements))\n",
    "    torch.cuda.empty_cache()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcionalidad compartida para los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_transformer_model(model : nn.Module):\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_accuracy(label, pred, tgt_pad_idx = PAD_ES_IDX):\n",
    "  pred = torch.argmax(pred, dim=-1)\n",
    "  match = label.eq(pred)\n",
    "  mask = label.ne(tgt_pad_idx)\n",
    "  match = match & mask\n",
    "  match = match.type(torch.float32) \n",
    "  mask =  mask.type(torch.float32)\n",
    "  return torch.sum(match)/torch.sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Basado en https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/transformer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Componentes comunes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mPositionalEncoding\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, emb_size: \u001b[38;5;28mint\u001b[39m, dropout: \u001b[38;5;28mfloat\u001b[39m, maxlen: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(PositionalEncoding, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer (single decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer (dual decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and evaluation routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Epoch Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq Dual Decoder Transfomer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and evaluation routines (dual decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Epoch Process (dual decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMB_SIZE = 256\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "base_EN_ES_model_path = 'transformer_base_en_es_{0}.pt'\n",
    "base_EN_FR_model_path = 'transformer_base_en_fr_{0}.pt'\n",
    "# custom_EN_ES_model_path = 'transformer_cstm_en_es_{0}.pt'\n",
    "# custom_EN_FR_model_path = 'transformer_cstm_en_fr_{0}.pt'\n",
    "dual_model_path = 'transformer_cstm_en_es_fr_{0}.pt'\n",
    "dual_model_path_WS = 'transformer_cstm_en_es_fr_weight_sharing_{0}.pt'\n",
    "\n",
    "transformer_base_EN_to_ES = Seq2SeqTransformer(\"Bilingual_EN_ES (Lib)\", NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, NHEAD, en_vocab_size, es_vocab_size, FFN_HID_DIM)\n",
    "transformer_base_EN_to_FR = Seq2SeqTransformer(\"Bilingual_EN_FR (Lib)\",NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, NHEAD, en_vocab_size, fr_vocab_size, FFN_HID_DIM)\n",
    "transformer_dual_EN_to_ES_and_FS = DoubleTaskSeq2SeqTransformer(\"Trilingual_EN_ES_FR\", NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, NHEAD, en_vocab_size, es_vocab_size, fr_vocab_size, FFN_HID_DIM)\n",
    "\n",
    "\n",
    "transformer_dual_EN_to_ES_and_FS_with_WS = DoubleTaskSeq2SeqTransformer(\"Trilingual_EN_ES_FR_WS\", NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, NHEAD, en_vocab_size, es_vocab_size, fr_vocab_size, FFN_HID_DIM, weight_sharing=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "transformer_base_EN_to_ES.apply(init_transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_params(transformer_base_EN_to_ES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "transformer_base_EN_to_FR.apply(init_transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_params(transformer_base_EN_to_FR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "transformer_dual_EN_to_ES_and_FS.apply(init_transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_params(transformer_dual_EN_to_ES_and_FS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "transformer_dual_EN_to_ES_and_FS_with_WS.apply(init_transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_params(transformer_dual_EN_to_ES_and_FS_with_WS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn_base_EN_to_ES = torch.nn.CrossEntropyLoss(ignore_index=PAD_ES_IDX)\n",
    "loss_fn_base_EN_to_FR = torch.nn.CrossEntropyLoss(ignore_index=PAD_ES_IDX)\n",
    "\n",
    "\n",
    "\n",
    "loss_fn_dual_decoder_EN_to_ES = torch.nn.CrossEntropyLoss(ignore_index=PAD_ES_IDX)\n",
    "loss_fn_dual_decoder_EN_to_FR = torch.nn.CrossEntropyLoss(ignore_index=PAD_FR_IDX)\n",
    "\n",
    "\n",
    "loss_fn_dual_decoder_EN_to_ES_WS = torch.nn.CrossEntropyLoss(ignore_index=PAD_ES_IDX)\n",
    "loss_fn_dual_decoder_EN_to_FR_WS = torch.nn.CrossEntropyLoss(ignore_index=PAD_FR_IDX)\n",
    "\n",
    "optimizer_base_EN_to_ES = torch.optim.Adam(transformer_base_EN_to_ES.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "optimizer_base_EN_to_FR = torch.optim.Adam(transformer_base_EN_to_FR.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "scheduler_EN_ES = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_base_EN_to_ES, 'min')\n",
    "scheduler_EN_FR = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_base_EN_to_FR, 'min')\n",
    "\n",
    "optimizer_custom_EN_to_ES_and_FR = torch.optim.Adam(transformer_dual_EN_to_ES_and_FS.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "optimizer_custom_EN_to_ES_and_FR_WS = torch.optim.Adam(transformer_dual_EN_to_ES_and_FS_with_WS.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "scheduler_EN_to_ES_and_FR = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_custom_EN_to_ES_and_FR, 'min')\n",
    "scheduler_EN_to_ES_and_FR_WS = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_custom_EN_to_ES_and_FR_WS, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# measure_data_sizing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución de los modelos basados en un único decodificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol, pad_symbol):\n",
    "\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = generate_square_subsequent_mask(ys.size(0)).type(torch.bool).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == pad_symbol:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate(model: nn.Module, src_sentence: str, src_lang : Language, tgt_lang : Language):\n",
    "    model.eval()\n",
    "\n",
    "    start_symbol = 0\n",
    "    pad_symbol = 0\n",
    "\n",
    "    src_vocab = None\n",
    "    tgt_vocab = None\n",
    "    src_tokenizer = None\n",
    "\n",
    "    match (src_lang):\n",
    "        case Language.EN:\n",
    "            src_vocab = en_vocabulary\n",
    "            src_tokenizer = en_tokenizer\n",
    "        case Language.ES:\n",
    "            src_vocab = es_vocabulary\n",
    "            src_tokenizer = es_tokenizer\n",
    "        case Language.FR:\n",
    "            src_vocab = fr_vocabulary\n",
    "            src_tokenizer = fr_tokenizer\n",
    "        case _:\n",
    "            raise RuntimeError(\"invalid src language\")\n",
    "\n",
    "    match (tgt_lang):\n",
    "        case Language.EN:\n",
    "            tgt_vocab = en_vocabulary\n",
    "            start_symbol = PAD_EN_IDX\n",
    "            pad_symbol = BOS_EN_IDX\n",
    "        case Language.ES:\n",
    "            tgt_vocab = es_vocabulary\n",
    "            start_symbol = PAD_ES_IDX\n",
    "            pad_symbol = BOS_ES_IDX\n",
    "        case Language.FR:\n",
    "            tgt_vocab = fr_vocabulary\n",
    "            start_symbol = PAD_FR_IDX\n",
    "            pad_symbol = BOS_FR_IDX\n",
    "        case _:\n",
    "            raise RuntimeError(\"invalid src language\")\n",
    "\n",
    "    src_seq = torch.Tensor([src_vocab[token] for token in src_tokenizer(clean_text(src_sentence))])\n",
    "    src_seq = src_seq.view(-1, 1)\n",
    "    num_tokens = src_seq.shape[0]\n",
    "    src_seq_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(model,  src_seq, src_seq_mask, max_len=num_tokens + 5, start_symbol=start_symbol, pad_symbol=pad_symbol).flatten()\n",
    "\n",
    "    return \" \".join(tgt_vocab.lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(BOS_WORD, \"\").replace(EOS_WORD, \"\")\n",
    "\n",
    "\n",
    "def translate_bleu(model: nn.Module, sequence):\n",
    "    model.eval()\n",
    "    start_symbol = 0\n",
    "    pad_symbol = 0\n",
    "    src_seq = torch.Tensor(sequence)\n",
    "    src_seq = src_seq.view(-1, 1)\n",
    "    num_tokens = src_seq.shape[0]\n",
    "    src_seq_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(model,  src_seq, src_seq_mask, max_len=num_tokens + 5, start_symbol=start_symbol, pad_symbol=pad_symbol).flatten()\n",
    "    return tgt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "max_printing_results = 5\n",
    "\n",
    "tst_models = [transformer_base_EN_to_ES, transformer_base_EN_to_FR]\n",
    "tst_slangs = [Language.EN, Language.EN] \n",
    "tst_tlangs = [Language.ES, Language.FR] \n",
    "\n",
    "\n",
    "for m, s_lang, t_lang in zip(tst_models, tst_slangs, tst_tlangs):\n",
    "    print (\"Model: {0}\".format(m.getName()))\n",
    "    exact = []\n",
    "    candidate = []\n",
    "    i = 0\n",
    "    for (src, tgt_ES, tgt_FR) in iter(tst_subset):\n",
    "\n",
    "        src_EN_tokens = en_vocabulary.lookup_tokens(src)\n",
    "        tgt_ES_tokens = es_vocabulary.lookup_tokens(tgt_ES)\n",
    "        tgt_FR_tokens = fr_vocabulary.lookup_tokens(tgt_FR)\n",
    "\n",
    "        src_txt = \" \".join(src_EN_tokens).replace(BOS_WORD, \"\").replace(EOS_WORD, \"\")\n",
    "        tgt_es_txt = \" \".join(tgt_ES_tokens).replace(BOS_WORD, \"\").replace(EOS_WORD, \"\")\n",
    "        tgt_fr_txt = \" \".join(tgt_FR_tokens).replace(BOS_WORD, \"\").replace(EOS_WORD, \"\")\n",
    "       \n",
    "        x = translate_bleu(m, src)\n",
    "\n",
    "        pred_txt = None\n",
    "        ground_truth = None\n",
    "        output_tokens = None\n",
    "        exact_tokens = None\n",
    "        if t_lang == Language.ES:\n",
    "            exact_tokens = tgt_ES_tokens\n",
    "            ground_truth = tgt_es_txt\n",
    "            output_tokens = es_vocabulary.lookup_tokens(list(x.cpu().numpy()))\n",
    "            pred_txt = \" \".join(output_tokens).replace(BOS_WORD, \"\").replace(EOS_WORD, \"\").replace(PAD_WORD, \" \")\n",
    "\n",
    "        if t_lang == Language.FR:\n",
    "            exact_tokens = tgt_FR_tokens\n",
    "            ground_truth = tgt_fr_txt\n",
    "            output_tokens = fr_vocabulary.lookup_tokens(list(x.cpu().numpy()))\n",
    "            pred_txt = \" \".join(output_tokens).replace(BOS_WORD, \"\").replace(EOS_WORD, \"\").replace(PAD_WORD, \" \")\n",
    "\n",
    "        if len(exact_tokens) > 0 and len(output_tokens) > 0:\n",
    "            exact.append(exact_tokens)\n",
    "            candidate.append(output_tokens)\n",
    "\n",
    "        if (i < max_printing_results):\n",
    "            print(\"{0}.SRC_SEQ: {1}\".format(i+1, src_txt))\n",
    "            print(\"{0}.TGT_SEQ: {1}\".format(i+1, ground_truth))\n",
    "            print(\"{0}.OUT_SEQ: {1}\".format(i+1, pred_txt))\n",
    "        i+=1\n",
    "\n",
    "        if (i > 1):\n",
    "            break\n",
    "\n",
    "            \n",
    "    print(candidate)\n",
    "    print(exact)\n",
    "    print(\"BLEU Score for {0} = {1}\".format(m.getName(), bleu_score(candidate, exact)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "display(result)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
