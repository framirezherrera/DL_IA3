{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"idal-logo.png\" align=\"right\" style=\"float\" width=\"400\">\n",
    "<font color=\"#CA3532\"><h1 align=\"left\">mIA3</h1></font>\n",
    "<font color=\"#6E6E6E\"><h2 align=\"left\">Tarea Evaluable. Aprendizaje Profundo 1 y 2 (Parte 2 de 3).</h2></font> \n",
    "\n",
    "#### Elaborado por Felipe Ramírez Herrera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install panda\n",
    "\n",
    "# !pip install torch\n",
    "# !pip install torchtext\n",
    "# !pip install torchmetrics\n",
    "# !pip install spacy\n",
    "\n",
    "# !pip install jupyter\n",
    "# !pip install ipywidgets\n",
    "\n",
    "# !pip install editdistance\n",
    "# !pip install six\n",
    "# !pip install typeguard\n",
    "\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn\n",
    "# !pip install scikit-learn\n",
    "\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download es_core_news_sm\n",
    "# !python -m spacy download fr_core_news_sm\n",
    "# !pip install wget\n",
    "\n",
    "# !pip install --upgrade --force-reinstall torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import csv\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
    "from torchtext.vocab import Vocab\n",
    "import logging\n",
    "import pandas as pd\n",
    "import six\n",
    "from typing import List, Tuple, Union\n",
    "from argparse import Namespace\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as mp\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formato de Mathplotlib\n",
    "\n",
    "mp.rc('font', size=8)\n",
    "mp.rc('axes', titlesize=8)\n",
    "mp.rc('axes', labelsize=8)\n",
    "mp.rc('xtick', labelsize=8)\n",
    "mp.rc('ytick', labelsize=8)\n",
    "mp.rc('legend', fontsize=8)\n",
    "mp.rc('figure', titlesize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cuda_memory_available():\n",
    "    if torch.cuda.is_available():\n",
    "        t = torch.cuda.get_device_properties(DEVICE).total_memory\n",
    "        r = torch.cuda.memory_reserved(DEVICE)\n",
    "        a = torch.cuda.memory_allocated(DEVICE)\n",
    "        f = (t - a - r) / t  # free inside reserved\n",
    "        print(\"CUDA Available Memory: {0}\".format(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model, return_int=False):\n",
    "    params = sum([torch.prod(torch.tensor(x.shape)).item() for x in model.parameters() if x.requires_grad])\n",
    "    if return_int:\n",
    "        return params\n",
    "    else:\n",
    "        print(\"There are {:,} trainable parameters in this model.\".format(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://conferences.unite.un.org/UNCorpus/Home/DownloadOverview\n",
    "# !cat /home/framirez/translation_multilingual/UNv1.0.6way.tar.gz.* | tar -xzf -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "\n",
    "# EN_corpus_file = \"UNv1.0.6way_en.zip\" # UN Parallel Corpus English (EN)\n",
    "# ES_corpus_file = \"UNv1.0.6way_es.zip\" # UN Parallel Corpus Spanish (ES)\n",
    "# FR_corpus_file = \"UNv1.0.6way_fr.zip\" # UN Parallel Corpus French (FR)\n",
    "\n",
    "# corpus_source_folder = \"/content/drive/MyDrive/\"\n",
    "# corpus_target_folder = \"/tmp/\"\n",
    "\n",
    "# if (not os.path.exists(corpus_target_folder + EN_corpus_file)):\n",
    "#   with zipfile.ZipFile(corpus_source_folder + EN_corpus_file,\"r\") as zip_ref:\n",
    "#       zip_ref.extractall(corpus_target_folder)\n",
    "\n",
    "# if (not os.path.exists(corpus_target_folder + ES_corpus_file)):\n",
    "#   with zipfile.ZipFile(corpus_source_folder + ES_corpus_file,\"r\") as zip_ref:\n",
    "#       zip_ref.extractall(corpus_target_folder)\n",
    "\n",
    "# if (not os.path.exists(corpus_target_folder + FR_corpus_file)):\n",
    "#   with zipfile.ZipFile(corpus_source_folder + FR_corpus_file,\"r\") as zip_ref:\n",
    "#       zip_ref.extractall(corpus_target_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the cuDNN backend for 16-bit training and inference for CUDA-enabled GPUs\n",
    "# torch.backends.cudnn.benchmark =  True\n",
    "# torch.backends.cudnn.enabled =  True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 64 # Re-calculated further \n",
    "min_seq_length = 16 # Re-calculated further "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUMBER_OF_EPOCHS = 2\n",
    "EARLY_STOPPING_EPOCHS = 3\n",
    "CLIPPING_VALUE = 15                   # clipping value, or None to prevent gradient clipping\n",
    "MAXIMUM_NUMBER_OF_SAMPLES = 70000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Lets GPU to get more cooler\n",
    "INNER_GPU_REST_TIME = 0 # Rest time between epochs\n",
    "OUTER_GPU_REST_TIME = 0 # Rest time between model training / validation processes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "\n",
    "def generate_examples(src_file, tgt_a_file, tgt_b_file):\n",
    "    data = []\n",
    "    with open(src_file, encoding=\"utf-8\") as src_f, open(tgt_a_file, encoding=\"utf-8\") as tgt_a, open(tgt_b_file, encoding=\"utf-8\") as tgt_b:\n",
    "        for idx, (a, b, c) in enumerate(zip(src_f, tgt_a, tgt_b)):           \n",
    "            if (a.isspace() | b.isspace() | c.isspace()):\n",
    "                continue\n",
    "            data.append({'text_en' : a, 'text_es' : b, 'text_fr' :c} )\n",
    "    return pd.DataFrame.from_records(data=data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from unicodedata import normalize\n",
    "\n",
    "PAD_WORD = '<pad>'\n",
    "UNK_WORD = '<unk>'  # Unknown token symbol\n",
    "BOS_WORD = '<bos>'  # Begin-of-Sentence token symbol\n",
    "EOS_WORD = '<eos>'  # End-of-Sentence token symbol\n",
    "\n",
    "def clean_text(text : str):\n",
    "    text = normalize('NFD', str(text).lower())\n",
    "    text = re.sub('[^A-Za-z ]+', '', text)\n",
    "    return text\n",
    "\n",
    "pkl_dataset_file = \"transformers_us_ds.pkl\"\n",
    "un_ds = pd.DataFrame()\n",
    "\n",
    "if os.path.exists(pkl_dataset_file):\n",
    "    un_ds = pd.read_pickle(pkl_dataset_file) \n",
    "else:\n",
    "    en_train_path = \"./mnt/drive/UNv1.0.6way.en\"\n",
    "    es_train_path = \"./mnt/drive/UNv1.0.6way.es\"\n",
    "    fr_train_path = \"./mnt/drive/UNv1.0.6way.fr\"\n",
    "    df = generate_examples(en_train_path, es_train_path, fr_train_path)\n",
    "    df['text_en'] = df['text_en'].apply(lambda row: clean_text(row))\n",
    "    df['text_es'] = df['text_es'].apply(lambda row: clean_text(row))\n",
    "    df['text_fr'] = df['text_fr'].apply(lambda row: clean_text(row))\n",
    "    un_ds = df\n",
    "    un_ds.to_pickle(pkl_dataset_file)\n",
    "len(un_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_ds.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_ds.head(-25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_ds = un_ds.head(MAXIMUM_NUMBER_OF_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "import torchtext\n",
    "\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "es_tokenizer = get_tokenizer('spacy', language='es_core_news_sm')\n",
    "fr_tokenizer = get_tokenizer('spacy', language='fr_core_news_sm')\n",
    "\n",
    "Language = Enum('Language', ['EN', 'ES', 'FR'])\n",
    "\n",
    "def yield_tokens(Lang: Language=Language.EN):\n",
    "    for index, row in un_ds.iterrows():     \n",
    "         if (Lang == Language.EN):\n",
    "            yield en_tokenizer(str(row[\"text_en\"]))  \n",
    "         if (Lang == Language.ES):\n",
    "            yield es_tokenizer(str(row[\"text_es\"]))\n",
    "         if (Lang == Language.FR):\n",
    "           yield fr_tokenizer(str(row[\"text_fr\"]))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab_file = 'transformers_vocab_en.pth'\n",
    "es_vocab_file = 'transformers_vocab_es.pth'\n",
    "fr_vocab_file = 'transformers_vocab_fr.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import torchtext.data\n",
    "import torchtext.data.datasets_utils\n",
    "import torchtext.datasets\n",
    "\n",
    "SPECIALS = [PAD_WORD, BOS_WORD, EOS_WORD,  UNK_WORD]\n",
    "\n",
    "en_vocabulary = None\n",
    "es_vocabulary = None\n",
    "fr_vocabulary = None\n",
    "\n",
    "if os.path.exists(en_vocab_file):\n",
    "    en_vocabulary = torch.load(en_vocab_file)\n",
    "else:\n",
    "    en_vocabulary = build_vocab_from_iterator(yield_tokens(Language.EN), specials=SPECIALS)\n",
    "    torch.save(en_vocabulary, en_vocab_file)\n",
    "\n",
    "if os.path.exists(es_vocab_file):\n",
    "    es_vocabulary = torch.load(es_vocab_file)\n",
    "else:\n",
    "    es_vocabulary = build_vocab_from_iterator(yield_tokens(Language.ES), specials=SPECIALS)\n",
    "    torch.save(es_vocabulary, es_vocab_file)\n",
    "\n",
    "if os.path.exists(fr_vocab_file):\n",
    "    fr_vocabulary = torch.load(fr_vocab_file)\n",
    "else:\n",
    "    fr_vocabulary = build_vocab_from_iterator(yield_tokens(Language.FR), specials=SPECIALS)\n",
    "    torch.save(fr_vocabulary, fr_vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EN Vocabulary Size = {0}\".format(len(en_vocabulary)))\n",
    "print(\"ES Vocabulary Size = {0}\".format(len(es_vocabulary)))\n",
    "print(\"FR Vocabulary Size = {0}\".format(len(fr_vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_EN_IDX = en_vocabulary[PAD_WORD]\n",
    "BOS_EN_IDX = en_vocabulary[BOS_WORD]\n",
    "EOS_EN_IDX = en_vocabulary[EOS_WORD]\n",
    "UNK_EN_IDX = en_vocabulary[UNK_WORD]\n",
    "\n",
    "\n",
    "PAD_ES_IDX = es_vocabulary[PAD_WORD]\n",
    "BOS_ES_IDX = es_vocabulary[BOS_WORD]\n",
    "EOS_ES_IDX = es_vocabulary[EOS_WORD]\n",
    "UNK_ES_IDX = es_vocabulary[UNK_WORD]\n",
    "\n",
    "PAD_FR_IDX = fr_vocabulary[PAD_WORD]\n",
    "BOS_FR_IDX = fr_vocabulary[BOS_WORD]\n",
    "EOS_FR_IDX = fr_vocabulary[EOS_WORD]\n",
    "UNK_FR_IDX = fr_vocabulary[UNK_WORD]\n",
    "\n",
    "en_vocab_size = len(en_vocabulary) \n",
    "es_vocab_size = len(es_vocabulary) \n",
    "fr_vocab_size = len(fr_vocabulary)\n",
    "\n",
    "print(\"EN: PAD = {0} BOS = {1} EOS = {2} UNK = {3}\".format(PAD_EN_IDX, BOS_EN_IDX, EOS_EN_IDX, UNK_EN_IDX))\n",
    "print(\"ES: PAD = {0} BOS = {1} EOS = {2} UNK = {3}\".format(PAD_ES_IDX, BOS_ES_IDX, EOS_ES_IDX, UNK_ES_IDX))\n",
    "print(\"FR: PAD = {0} BOS = {1} EOS = {2} UNK = {3}\".format(PAD_FR_IDX, BOS_FR_IDX, EOS_FR_IDX, UNK_FR_IDX))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate(tokenized_text, allocate : bool = True,  pad_index : int = PAD_EN_IDX, bos_index : int = BOS_EN_IDX, eos_index : int = EOS_EN_IDX):  \n",
    "    result = []\n",
    "    if len(tokenized_text) < max_seq_length:\n",
    "        if (allocate):\n",
    "            result = [bos_index] + tokenized_text + [eos_index]\n",
    "            left = max_seq_length - len(result)\n",
    "            padding = [pad_index] * left\n",
    "            result = result + padding\n",
    "        else:\n",
    "            left = max_seq_length - len(tokenized_text)\n",
    "            padding = [pad_index] * left\n",
    "            result = tokenized_text + padding       \n",
    "    else:\n",
    "        raise Exception(\"pad_or_truncate: max_seq_length not computed properly\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_file = 'transformers_full.pth'\n",
    "trn_data_file = 'transformers_trn.pth'\n",
    "val_data_file = 'transformers_val.pth'\n",
    "tst_data_file = 'transformers_tst.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(un_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "from collections import Counter \n",
    "\n",
    "MINIMUM_ALLOWED_SIZE_OF_SEQ = 5\n",
    "MAXIMUM_ALLOWED_SIZE_OF_SEQ = 100\n",
    "\n",
    "full_data = []\n",
    "trn_subset, val_subset, tst_subset = [], [], []\n",
    "\n",
    "random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "max_len = 0\n",
    "min_len = 4500\n",
    "\n",
    "en_counter = Counter()\n",
    "es_counter = Counter()\n",
    "fr_counter = Counter()\n",
    "\n",
    "en_lengths = []\n",
    "es_lengths = []\n",
    "fr_lengths = []\n",
    "\n",
    "fe = os.path.exists(full_data_file)\n",
    "te = os.path.exists(trn_data_file)\n",
    "ve = os.path.exists(val_data_file)\n",
    "tt = os.path.exists(tst_data_file)\n",
    "\n",
    "if fe and te and ve and tt:\n",
    "    with open(full_data_file, 'rb') as fp:\n",
    "            full_data = pickle.load(fp)\n",
    "    with open(trn_data_file, 'rb') as fp:\n",
    "            trn_subset = pickle.load(fp)\n",
    "    with open(val_data_file, 'rb') as fp:\n",
    "            val_subset = pickle.load(fp)        \n",
    "    with open(tst_data_file, 'rb') as fp:\n",
    "            tst_subset = pickle.load(fp)           \n",
    "    for (en_seq,es_seq,fr_seq) in full_data:     \n",
    "        en_counter.update(en_seq)\n",
    "        es_counter.update(es_seq)\n",
    "        fr_counter.update(fr_seq)\n",
    "        a = len(en_seq)\n",
    "        b = len(es_seq)\n",
    "        c = len(fr_seq) \n",
    "        en_lengths.append(a)\n",
    "        es_lengths.append(b)\n",
    "        fr_lengths.append(c)\n",
    "        max_len = max(max_len, a, b, c)    \n",
    "        min_len = min(min_len, a, b, c)    \n",
    "else:\n",
    "    for idx, row in un_ds.iterrows():\n",
    "        \n",
    "        en_exp = row[\"text_en\"].rstrip(\"\\n\")\n",
    "        es_exp = row[\"text_es\"].rstrip(\"\\n\")\n",
    "        fr_exp = row[\"text_fr\"].rstrip(\"\\n\")\n",
    "\n",
    "        en_seq = [en_vocabulary[token] for token in en_tokenizer(en_exp)]\n",
    "        es_seq = [es_vocabulary[token] for token in es_tokenizer(es_exp)]\n",
    "        fr_seq = [fr_vocabulary[token] for token in fr_tokenizer(fr_exp)]\n",
    "\n",
    "        a = len(en_seq)\n",
    "        b = len(es_seq)\n",
    "        c = len(fr_seq) \n",
    "\n",
    "        seq_min = min(a, b, c)\n",
    "        seq_max = max(a, b, c)\n",
    "\n",
    "        if seq_min  >= MINIMUM_ALLOWED_SIZE_OF_SEQ and seq_max <= MAXIMUM_ALLOWED_SIZE_OF_SEQ: \n",
    "            \n",
    "            en_counter.update(en_seq)\n",
    "            es_counter.update(es_seq)\n",
    "            fr_counter.update(fr_seq)\n",
    "       \n",
    "            en_lengths.append(a)\n",
    "            es_lengths.append(b)\n",
    "            fr_lengths.append(c)\n",
    "\n",
    "            max_len = max(max_len, seq_max)\n",
    "            min_len = min(min_len, seq_min) \n",
    "            full_data.append((en_seq, es_seq, fr_seq))\n",
    "\n",
    "    trn_subset, val_subset = train_test_split(full_data, test_size=0.3, train_size=0.7, random_state=1234, shuffle=True)\n",
    "    val_subset, tst_subset = train_test_split(val_subset, test_size=0.33, train_size=0.67, random_state=1234, shuffle=True)\n",
    "\n",
    "    with open(full_data_file, 'wb') as fp:\n",
    "        pickle.dump(full_data, fp)\n",
    "    with open(trn_data_file, 'wb') as fp:\n",
    "        pickle.dump(trn_subset, fp)\n",
    "    with open(val_data_file, 'wb') as fp:\n",
    "        pickle.dump(val_subset, fp)\n",
    "    with open(tst_data_file, 'wb') as fp:\n",
    "        pickle.dump(tst_subset, fp)\n",
    "\n",
    "if (max_seq_length < max_len):\n",
    "    max_seq_length = max_len + 2\n",
    "\n",
    "if (min_seq_length > min_len):\n",
    "    min_seq_length = min_len + 2\n",
    "\n",
    "\n",
    "print(\"Selected Records: {0}\".format(len(full_data)))\n",
    "print(\"MAX SEQ LEN {0}\".format(max_seq_length))\n",
    "print(\"MIN SEQ LEN {0}\".format(min_seq_length))\n",
    "\n",
    "size_of_trn_set_size = len(trn_subset)\n",
    "size_of_val_set_size = len(val_subset)\n",
    "size_of_tst_set_size = len(tst_subset)\n",
    "\n",
    "print(\"Training tuples: {0} Validation tuples: {1} Testing tuples: {2}\".format(size_of_trn_set_size, size_of_val_set_size, size_of_tst_set_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = mp.figure(figsize=(8, 10))\n",
    "ax0 = fig.add_subplot(311)\n",
    "ax0.hist(en_lengths, rwidth=0.8, color='gray')\n",
    "ax0.set_title(\"Longitud de las oraciones (EN)\")\n",
    "ax0.set_xlabel(\"Cantidad de tokens\")\n",
    "\n",
    "ax1 = fig.add_subplot(312)\n",
    "ax1.hist(es_lengths, rwidth=0.8, color='gray')\n",
    "ax1.set_title(\"Longitud de las oraciones (ES)\")\n",
    "ax1.set_xlabel(\"Cantidad de tokens\")\n",
    "\n",
    "ax2 = fig.add_subplot(313)\n",
    "ax2.hist(fr_lengths, rwidth=0.8, color='gray')\n",
    "ax2.set_title(\"Longitud de las oraciones (FR)\")\n",
    "ax2.set_xlabel(\"Cantidad de tokens\")\n",
    "\n",
    "mp.tight_layout()\n",
    "mp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.figure(figsize=(8,6))\n",
    "mp.hist2d(en_lengths, es_lengths, bins=max_seq_length-2, cmap='binary')\n",
    "mp.title(\"Joint Distribution of Sentence Lengths\")\n",
    "mp.xlabel(\"# English Tokens\")\n",
    "mp.ylabel(\"# Spanish Tokens\")\n",
    "mp.colorbar()\n",
    "mp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.figure(figsize=(8,6))\n",
    "mp.hist2d(en_lengths, fr_lengths, bins=max_seq_length-2, cmap='binary')\n",
    "mp.title(\"Joint Distribution of Sentence Lengths\")\n",
    "mp.xlabel(\"# English Tokens\")\n",
    "mp.ylabel(\"# French Tokens\")\n",
    "mp.colorbar()\n",
    "mp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(counter : Counter, vocab: Vocab, k=20, ax=None):\n",
    "    top_k = counter.most_common(k)\n",
    "    tokens, freqs = zip(*reversed(top_k))\n",
    "    \n",
    "    words = [vocab.lookup_token(token) for token in tokens]\n",
    "\n",
    "\n",
    "    if ax is None:\n",
    "        mp.barh(words, freqs, color='gray')\n",
    "    else:\n",
    "        ax.barh(words, freqs, color='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = mp.figure(figsize=(14, 8))\n",
    "\n",
    "ax0 = fig.add_subplot(131)\n",
    "plot_top_words(en_counter, en_vocabulary, ax=ax0)\n",
    "ax0.set_title(\"Top 20 English Words\")\n",
    "ax0.set_xlabel(\"Raw Frequency\")\n",
    "\n",
    "ax1 = fig.add_subplot(132)\n",
    "plot_top_words(es_counter,es_vocabulary, ax=ax1)\n",
    "ax1.set_title(\"Top 20 Spanish Words\")\n",
    "ax1.set_xlabel(\"Raw Frequency\")\n",
    "\n",
    "ax2 = fig.add_subplot(133)\n",
    "plot_top_words(fr_counter,fr_vocabulary, ax=ax2)\n",
    "ax2.set_title(\"Top 20 French Words\")\n",
    "ax2.set_xlabel(\"Raw Frequency\")\n",
    "\n",
    "mp.tight_layout()\n",
    "mp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from  torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "for en_id, es_id, fr_id in zip(en_vocabulary.lookup_indices(SPECIALS), es_vocabulary.lookup_indices(SPECIALS), fr_vocabulary.lookup_indices(SPECIALS)):\n",
    "  assert en_id == es_id & es_id == fr_id\n",
    "\n",
    "# Importante: Solo cargo los datos al dispositivo (GPU) cuando voy a procesarlos\n",
    "# es un trade-off entre performance del entrenamiento y capacidad de carga de los\n",
    "# datos dada las limitaciones de los GPU de consumo (Nvidia RTX):\n",
    "\n",
    "def tensor_transform(token_ids: List[int], bos_idx, eos_idx): \n",
    "    list = [bos_idx] + token_ids + [eos_idx]                         \n",
    "    return torch.as_tensor(list, device=DEVICE)\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "    en_batch, es_batch, fr_batch = [], [], []\n",
    "    for (en_item, es_item, fr_item) in data_batch:     \n",
    "        en_t = tensor_transform(en_item, BOS_EN_IDX, EOS_EN_IDX)\n",
    "        es_t = tensor_transform(es_item, BOS_ES_IDX, EOS_ES_IDX)\n",
    "        fr_t = tensor_transform(fr_item, BOS_FR_IDX, EOS_FR_IDX)     \n",
    "        en_batch.append(en_t)\n",
    "        es_batch.append(es_t)\n",
    "        fr_batch.append(fr_t)\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_EN_IDX, batch_first=False)\n",
    "    es_batch = pad_sequence(es_batch, padding_value=PAD_ES_IDX, batch_first=False)\n",
    "    fr_batch = pad_sequence(fr_batch, padding_value=PAD_FR_IDX, batch_first=False)\n",
    "    return en_batch, es_batch, fr_batch\n",
    "\n",
    "def generate_batch_EN_ES(data_batch):\n",
    "    en_batch, es_batch = [], []\n",
    "    for (en_item, es_item, _) in data_batch:     \n",
    "        en_t = tensor_transform(en_item, BOS_EN_IDX, EOS_EN_IDX)\n",
    "        es_t = tensor_transform(es_item, BOS_ES_IDX, EOS_ES_IDX)\n",
    "        en_batch.append(en_t)\n",
    "        es_batch.append(es_t)\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_EN_IDX, batch_first=False)\n",
    "    es_batch = pad_sequence(es_batch, padding_value=PAD_ES_IDX, batch_first=False)\n",
    "    return en_batch, es_batch\n",
    "\n",
    "def generate_batch_EN_FR(data_batch):\n",
    "    en_batch, fr_batch = [], []\n",
    "    for (en_item, _, fr_item) in data_batch:        \n",
    "        en_t = tensor_transform(en_item, BOS_EN_IDX, EOS_EN_IDX)\n",
    "        fr_t = tensor_transform(fr_item, BOS_FR_IDX, EOS_FR_IDX)\n",
    "        en_batch.append(en_t)\n",
    "        fr_batch.append(fr_t)\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_EN_IDX, batch_first=False)\n",
    "    fr_batch = pad_sequence(fr_batch, padding_value=PAD_FR_IDX, batch_first=False)\n",
    "    return en_batch, fr_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LanguageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, subset, length):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.length = length\n",
    "        self.subset = subset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.subset[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Bilingual_EN_to_ES_trnset = DataLoader(LanguageDataset(trn_subset, size_of_trn_set_size), batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch_EN_ES, num_workers=16, pin_memory=True)\n",
    "Bilingual_EN_to_ES_valset = DataLoader(LanguageDataset(val_subset, size_of_val_set_size), batch_size=BATCH_SIZE, shuffle=False, collate_fn=generate_batch_EN_ES, num_workers=8, pin_memory=True)\n",
    "Bilingual_EN_to_ES_tstset = DataLoader(LanguageDataset(tst_subset, size_of_tst_set_size), batch_size=BATCH_SIZE, shuffle=False, collate_fn=generate_batch_EN_ES, num_workers=8, pin_memory=True)\n",
    "\n",
    "Bilingual_EN_to_FR_trnset = DataLoader(LanguageDataset(trn_subset, size_of_trn_set_size), batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch_EN_FR, num_workers=16, pin_memory=True)\n",
    "Bilingual_EN_to_FR_valset = DataLoader(LanguageDataset(val_subset, size_of_val_set_size), batch_size=BATCH_SIZE, shuffle=False, collate_fn=generate_batch_EN_FR, num_workers=8, pin_memory=True)\n",
    "Bilingual_EN_to_FR_tstset = DataLoader(LanguageDataset(tst_subset, size_of_tst_set_size), batch_size=BATCH_SIZE, shuffle=False, collate_fn=generate_batch_EN_FR, num_workers=8, pin_memory=True)\n",
    "\n",
    "Trilingual_trnset = DataLoader(LanguageDataset(trn_subset, size_of_trn_set_size), batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch, num_workers=16, pin_memory=True)\n",
    "Trilingual_valset = DataLoader(LanguageDataset(val_subset, size_of_val_set_size), batch_size=BATCH_SIZE, shuffle=False, collate_fn=generate_batch, num_workers=8, pin_memory=True)\n",
    "Trilingual_tstset = DataLoader(LanguageDataset(tst_subset, size_of_tst_set_size), batch_size=BATCH_SIZE, shuffle=False, collate_fn=generate_batch, num_workers=8, pin_memory=True)\n",
    "\n",
    "batches_for_training = math.ceil(size_of_trn_set_size / BATCH_SIZE)\n",
    "batches_for_validation = math.ceil(size_of_val_set_size / BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "size_to_MB = 1024 * 1024\n",
    "\n",
    "def getSizeOf(a : torch.Tensor):\n",
    "    return sys.getsizeof(a) + torch.numel(a) * a.element_size()\n",
    "\n",
    "def ElementsOf(a : torch.Tensor):\n",
    "    return torch.numel(a)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.set_start_method('spawn', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
    "\n",
    "def calculate_dataset_memory_overhead(rank, data):\n",
    "\n",
    "    print(\"Data Processs ID: {}\".format(rank))\n",
    "\n",
    "    total = 0\n",
    "\n",
    "    for (x, tgt_ES, tgt_FR) in iter(data):\n",
    "\n",
    "        total += getSizeOf(x) / size_to_MB\n",
    "        total += getSizeOf(tgt_ES) / size_to_MB\n",
    "        total += getSizeOf(tgt_FR) / size_to_MB\n",
    "        del x\n",
    "        del tgt_ES\n",
    "        del tgt_FR\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"Tamaño estimado del conjunto de datos: {} Mbytes\".format(total))\n",
    "\n",
    "\n",
    "class ParallelLoader(DataLoader):\n",
    "    def __init__(self, dataset, parallelization_degree, rank, collate_fn, sampler):\n",
    "        super().__init__(dataset=dataset, sampler = sampler , collate_fn = collate_fn, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "parallelization_degree = 8\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    processes = []\n",
    "\n",
    "    for rank in range(1, parallelization_degree):\n",
    "\n",
    "        sampler = DistributedSampler(dataset=trn_subset, num_replicas=parallelization_degree, rank= rank)\n",
    "\n",
    "        data = ParallelLoader(LanguageDataset(trn_subset, size_of_trn_set_size), parallelization_degree, rank, generate_batch_EN_ES, sampler)\n",
    "\n",
    "        p = mp.Process(target=calculate_dataset_memory_overhead, args=[rank, data])\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# torch.multiprocessing.spawn.spawn(calculate_dataset_memory_overhead, args=(), nprocs=8, join=True, daemon=False, start_method='spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_transformer_model(model : nn.Module):\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_accuracy(label, pred, tgt_pad_idx = PAD_ES_IDX):\n",
    "  pred = torch.argmax(pred, dim=-1)\n",
    "  match = label.eq(pred)\n",
    "  mask = label.ne(tgt_pad_idx)\n",
    "  match = match & mask\n",
    "  match = match.type(torch.float32) \n",
    "  mask =  mask.type(torch.float32)\n",
    "  return torch.sum(match)/torch.sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def let_gpu_rest(minutes):\n",
    "    if minutes > 0:\n",
    "        time.sleep(minutes * 60)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fragmento de código. Funciones que despliegan las curvas de desempeño para el modelo\n",
    "# Se reutilizan a lo largo del ejercicio.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def plot_accuracy_curve(name, history):\n",
    "    fig, ax = plt.subplots(figsize=(PLOT_X, PLOT_Y), layout='constrained')\n",
    "    ax.set_title('Exactitud del modelo {0}'.format(name))\n",
    "    ax.plot(history['train_acc'], label='Entrenamiento', linestyle='solid', lw=2.5)\n",
    "    ax.plot(history['valid_acc'], label='Validación', linestyle='dashed', lw=2.5)\n",
    "    ax.set_xticks(np.arange(1, NUMBER_OF_EPOCHS + 1, 1))\n",
    "    ax.set_yticks(np.arange(0, 1, 1 / 10))\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Exactitud')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss_curve(name, history):\n",
    "    fig, ax = plt.subplots(figsize=(PLOT_X, PLOT_Y), layout='constrained')\n",
    "    ax.set_title('Pérdida del modelo {0}'.format(name))\n",
    "    ax.plot(history['train_loss'], label='Entrenamiento', linestyle='solid', lw=2.5)\n",
    "    ax.plot(history['valid_loss'], label='Validación', linestyle='dashed', lw=2.5)\n",
    "    ax.set_xticks(np.arange(1, NUMBER_OF_EPOCHS + 1, 1))\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Pérdida')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_pplx_curve(name, history):\n",
    "    fig, ax = plt.subplots(figsize=(PLOT_X, PLOT_Y), layout='constrained')\n",
    "    ax.set_title('Perplejidad del modelo {0}'.format(name))\n",
    "    ax.plot(history['train_ppl'], label='Entrenamiento', linestyle='solid', lw=2.5)\n",
    "    ax.plot(history['valid_ppl'], label='Validación', linestyle='dashed', lw=2.5)\n",
    "    ax.set_xticks(np.arange(1, NUMBER_OF_EPOCHS + 1, 1))\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Perplejidad')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_accuracy_curve_dual_transformer(name, history):\n",
    "    fig, ax = plt.subplots(figsize=(PLOT_X, PLOT_Y), layout='constrained')\n",
    "    ax.set_title('Exactitud del modelo {0}'.format(name))\n",
    "    ax.plot(history['train_conjugate_acc'], label='Entrenamiento', linestyle='solid', lw=2.5)\n",
    "    ax.plot(history['valid_conjugate_acc'], label='Validación', linestyle='dashed', lw=2.5)\n",
    "    ax.set_xticks(np.arange(1, NUMBER_OF_EPOCHS + 1, 1))\n",
    "    ax.set_yticks(np.arange(0, 1, 1 / 10))\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Exactitud')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_loss_curve_dual_transformer(name, history):\n",
    "    fig, ax = plt.subplots(figsize=(PLOT_X, PLOT_Y), layout='constrained')\n",
    "    ax.set_title('Pérdida del modelo {0}'.format(name))\n",
    "    ax.plot(history['train_conjugate_loss'], label='Entrenamiento', linestyle='solid', lw=2.5)\n",
    "    ax.plot(history['valid_conjugate_loss'], label='Validación', linestyle='dashed', lw=2.5)\n",
    "    ax.set_xticks(np.arange(1, NUMBER_OF_EPOCHS + 1, 1))\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Pérdida')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy_curve_dual_transformer_both(name, history):\n",
    "    fig, ax = plt.subplots(figsize=(PLOT_X, PLOT_Y), layout='constrained')\n",
    "    ax.set_title('Exactitud del modelo {0}'.format(name))\n",
    "    ax.plot(history['train_acc_a'], label='Entrenamiento (EN-ES)')\n",
    "    ax.plot(history['train_acc_b'], label='Entrenamiento (EN-FR)')\n",
    "    ax.plot(history['valid_acc_a'], label='Validación (EN-ES)')\n",
    "    ax.plot(history['valid_acc_b'], label='Validación (EN-FR)')\n",
    "    ax.set_xticks(np.arange(1, NUMBER_OF_EPOCHS + 1, 1))\n",
    "    ax.set_yticks(np.arange(0, 1, 1 / 10))\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Exactitud')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss_curve_dual_transformer_both(name, history):\n",
    "    fig, ax = plt.subplots(figsize=(PLOT_X, PLOT_Y), layout='constrained')\n",
    "    ax.set_title('Pérdida del modelo {0}'.format(name))\n",
    "    ax.plot(history['train_loss_a'], label='Entrenamiento (EN-ES)')\n",
    "    ax.plot(history['train_loss_b'], label='Entrenamiento (EN-FR)')\n",
    "    ax.plot(history['valid_loss_a'], label='Validación (EN-ES)')\n",
    "    ax.plot(history['valid_loss_b'], label='Validación (EN-FR)')\n",
    "    ax.set_xticks(np.arange(1, NUMBER_OF_EPOCHS + 1, 1))\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Pérdida')\n",
    "    ax.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq-to-Seq con GCNN\n",
    "\n",
    "Véase: https://arxiv.org/abs/1612.08083"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_ES_VOCAB_DIM = en_vocab_size\n",
    "OUTPUT_ES_VOCAB_DIM = es_vocab_size\n",
    "OUTPUT_FR_VOCAB_DIM = fr_vocab_size\n",
    "GCNN_EMBDIM = 256\n",
    "GCNN_HID_DIM = 512 # each conv. layer has 2 * hid_dim filters\n",
    "GCNN_ENC_LAYERS = 5 # number of conv. blocks in encoder\n",
    "GCNN_DEC_LAYERS = 5 # number of conv. blocks in decoder\n",
    "GCNN_ENC_KERNEL_SIZE = 3 # must be odd!\n",
    "GCNN_DEC_KERNEL_SIZE = 3 # can be even or odd\n",
    "GCNN_ENC_DROPOUT = 0.25\n",
    "GCNN_DEC_DROPOUT = 0.25\n",
    "\n",
    "CNN_TRG_PAD_IDX = PAD_ES_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_EN_ES_model_path = 'gcnn_mt_en_es_{0}.pt'\n",
    "CNN_EN_FR_model_path = 'gcnn_mt__en_fr_{0}.pt'\n",
    "CNN_EN_ES_FR_model_path = 'gcnn_mt__en_es_fr_{0}.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta función genera una matriz de tensores que representan las posiciones en una secuencia. \n",
    "# Es una función auxiliar utilizada para crear las embeddings posicionales necesarias para el modelo\n",
    "def create_postional_tensor(length, batch_size, device): \n",
    "    return torch.arange(0, length, device=device).unsqueeze(0).repeat(batch_size, 1) # [0, 1, 2, 3, ..., length - 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASE: GatedConvEncoder\n",
    "# Esta es una clase que define un módulo de encoder para una arquitectura Encoder - Decoder que utiliza convoluciones. \n",
    "# Basado en convoluciones que utiliza estas embeddings posicionales y realiza convoluciones, activaciones GLU y conexiones \n",
    "# residuales para procesar la entrada y generar salidas.\n",
    "\n",
    "# Based on https://github.com/facebookresearch/fairseq/blob/main/fairseq/\n",
    "\n",
    "class GatedConvEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, kernel_size, dropout, device, max_length = max_seq_length):\n",
    "        super().__init__()\n",
    "        assert kernel_size % 2 == 1, \"Invalid kernel size\"\n",
    "        \n",
    "        self.scale = math.sqrt(0.5) \n",
    "        self.tok_embedding = nn.Embedding(input_dim, emb_dim)   # token embedding layer\n",
    "        self.pos_embedding = nn.Embedding(max_length, emb_dim)  # positional embedding layer\n",
    "        self.incoming_projection = nn.Linear(emb_dim, hid_dim)\n",
    "        self.outgoing_projection = nn.Linear(hid_dim, emb_dim)\n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, out_channels = 2 * hid_dim, kernel_size = kernel_size, padding = (kernel_size - 1) // 2) for _ in range(n_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.device = device\n",
    "\n",
    "    # x = [batch size, x len]  \n",
    "    def forward(self, x):        \n",
    "        batch_size, src_len = x.shape[0], x.shape[1]   \n",
    "        pos = create_postional_tensor(src_len, batch_size, self.device) # [batch size, x len]       \n",
    "        # Calcula las embeddings de los tokens y las embeddings posicionales.\n",
    "        te = self.tok_embedding(x)      # [BS, X, emb dim]\n",
    "        pe = self.pos_embedding(pos)    # [BS, X, emb dim]\n",
    "        # Combina las embeddings sumándolas.\n",
    "        embedded = self.dropout(te + pe)                # [BS, X, emb dim]\n",
    "        # Proyecta las embeddings combinadas a una dimensión oculta.\n",
    "        conv_input = self.incoming_projection(embedded) # [BS, X, hid dim]\n",
    "        # Aplica capas de convolución, activación GLU y conexiones residuales.\n",
    "        conv_input = conv_input.permute(0, 2, 1)        # [BS, hid dim, X]  \n",
    "        for conv in self.convs:\n",
    "            # Pass through convolutional layer\n",
    "            conved = conv(self.dropout(conv_input))         # [BS, 2 * hid dim, X]\n",
    "            # Pass through GLU activation function\n",
    "            # GLU is far more stable than ReLU and learns faster than sigmoid.\n",
    "            conved = F.glu(conved, dim = 1)                 # [batch size, hid dim, src len]            \n",
    "            # Apply residual connection\n",
    "            conved = (conved + conv_input) * self.scale     # [batch size, hid dim, src len]\n",
    "            # Set conv_input to conved for next loop iteration\n",
    "            conv_input = conved       \n",
    "        #permute and convert back to emb dim\n",
    "        conved = self.outgoing_projection(conved.permute(0, 2, 1))  # [batch size, src len, emb dim]\n",
    "        combined = (conved + embedded) * self.scale                 # combined = [batch size, src len, emb dim]\n",
    "        return conved, combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on https://github.com/facebookresearch/fairseq/blob/main/fairseq/\n",
    "\n",
    "class GatedConvDecoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, kernel_size, dropout, trg_pad_idx, device, max_length = max_seq_length):\n",
    "        super().__init__()   \n",
    "        self.kernel_size = kernel_size\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        self.scale = self.scale = math.sqrt(0.5)  \n",
    "        self.tok_embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n",
    "         # projects from output of convolution to embedding dimension\n",
    "        self.dec_incoming_projection = nn.Linear(emb_dim, hid_dim)\n",
    "        # projects from embedding dimension to convolution size\n",
    "        self.dec_outgoing_projection = nn.Linear(hid_dim, emb_dim)       \n",
    "        # projects from output of convolution to embedding dimension\n",
    "        self.attn_incoming_projection = nn.Linear(hid_dim, emb_dim)\n",
    "        # projects from embedding dimension to convolution size\n",
    "        self.attn_outgoing_projection = nn.Linear(emb_dim, hid_dim) \n",
    "        # project back to size of vocabulary \n",
    "        self.fc_out = nn.Linear(emb_dim, output_dim)\n",
    "        # temporal convolutions\n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, out_channels = 2 * hid_dim, kernel_size = kernel_size) for _ in range(n_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    # Computes attention\n",
    "    # embedded          [batch size, trg len, emb dim]\n",
    "    # conved            [batch size, hid dim, trg len]\n",
    "    # encoder_conved    [batch size, src len, emb dim]\n",
    "    # encoder_combined  [batch size, src len, emb dim]\n",
    "        \n",
    "    def calculate_attention(self, embedded, conved, encoder_conved, encoder_combined):\n",
    "        #permute and convert back to emb dim\n",
    "        conved_emb = self.attn_incoming_projection(conved.permute(0, 2, 1))             # [batch size, trg len, emb dim]\n",
    "        combined = (conved_emb + embedded) * self.scale                                 # [batch size, trg len, emb dim]     \n",
    "        energy = torch.matmul(combined, encoder_conved.permute(0, 2, 1))                # [batch size, trg len, src len]\n",
    "        attention = F.softmax(energy, dim=2)                                            # [batch size, trg len, src len]\n",
    "        attended_encoding = torch.matmul(attention, encoder_combined)                   # [batch size, trg len, emd dim]\n",
    "        #convert from emb dim -> hid dim\n",
    "        attended_encoding = self.attn_outgoing_projection(attended_encoding)            # [batch size, trg len, hid dim]\n",
    "        #apply residual connection\n",
    "        attended_combined = (conved + attended_encoding.permute(0, 2, 1)) * self.scale  # [batch size, hid dim, trg len]\n",
    "        return attention, attended_combined\n",
    "\n",
    "    # trg               [trg len, batch size]\n",
    "    # encoder_conved    [batch size, src len, emb dim]\n",
    "    # encoder_combined  [batch size, src len, emb dim]\n",
    "\n",
    "    def forward(self, trg, encoder_conved, encoder_combined):              \n",
    "        batch_size, trg_len = trg.shape[0], trg.shape[1]   \n",
    "        #create position tensor\n",
    "        pos = create_postional_tensor(trg_len, batch_size, self.device)                         # [trg len, batch size]\n",
    "        #embed tokens and positions\n",
    "        tok_embedded = self.tok_embedding(trg)                                                  # [trg len, batch size, emb dim]\n",
    "        pos_embedded = self.pos_embedding(pos)                                                  # [trg len, batch size, emb dim]\n",
    "        embedded = self.dropout(tok_embedded + pos_embedded)                                    # [batch size, trg len, emb dim]\n",
    "        # project to size of convolution\n",
    "        conv_input = self.dec_incoming_projection(embedded)                                     # [batch size, trg len, hid dim]      \n",
    "        # permute for convolutional layer\n",
    "        conv_input = conv_input.permute(0, 2, 1)                                                # [batch size, hid dim, trg len]\n",
    "        batch_size = conv_input.shape[0]\n",
    "        hid_dim = conv_input.shape[1]\n",
    "        for conv in self.convs:\n",
    "            conv_input = self.dropout(conv_input)\n",
    "            padding = torch.zeros(batch_size, hid_dim, self.kernel_size - 1, device=self.device).fill_(self.trg_pad_idx)   \n",
    "            padded_conv_input = torch.cat((padding, conv_input), dim = 2) # [batch size, hid dim, trg len + kernel size - 1]\n",
    "            conved = conv(padded_conv_input)                              # [batch size, 2 * hid dim, trg len]\n",
    "            conved = F.glu(conved, dim = 1)                               # [batch size, hid dim, trg len]\n",
    "            attention, conved = self.calculate_attention(embedded, conved, encoder_conved, encoder_combined)  # [batch size, trg len, src len]\n",
    "            conved = (conved + conv_input) * self.scale                   # [batch size, hid dim, trg len]\n",
    "            conv_input = conved          \n",
    "        conved = self.dec_outgoing_projection(conved.permute(0, 2, 1))      # [batch size, trg len, emb dim]\n",
    "        output = self.fc_out(self.dropout(conved))                          # [batch size, trg len, output dim]\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-Encode 1-Decoder Seq2Seq using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedConvSeq2Seq(nn.Module):\n",
    "    def __init__(self, name, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    # x = [batch size, x len]\n",
    "    # y = [batch size, y len - 1]\n",
    "    def forward(self, x, y):\n",
    "        encoder_conved, encoder_combined = self.encoder(x)\n",
    "        output, attention = self.decoder(y, encoder_conved, encoder_combined) # [batch size, trg len - 1, output dim], [batch size, trg len - 1, src len]        \n",
    "        return output, attention\n",
    "    def getName(self):\n",
    "        return self.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-Encode 2-Decoder Seq2Seq using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedConvDualTaskSeq2Seq(nn.Module):\n",
    "    def __init__(self, name, encoder, decoder_a, decoder_b):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.encoder = encoder\n",
    "        self.decoder_a = decoder_a # Decoder for Task No. 1 (EN to ES)\n",
    "        self.decoder_b = decoder_b # Decoder for Task No. 2 (EN to FR)\n",
    "\n",
    "    # x = [batch size, src len]\n",
    "    # y0 = [batch size, y0 len - 1] (<bos> token sliced off without <eos>)\n",
    "    # y1 = [batch size, y1 len - 1] (<bos> token sliced off without <eos>) \n",
    "    def forward(self, x, y0, y1, returns_attention = False):         \n",
    "        encoder_conved, encoder_combined = self.encoder(x) # [batch size, src len, emb dim], [batch size, src len, emb dim]\n",
    "        output_a, attention_a = self.decoder_a(y0, encoder_conved, encoder_combined) # [batch size, y0 len - 1, output dim], attention_a = [batch size, y0 len - 1, x len]\n",
    "        output_b, attention_b = self.decoder_b(y1, encoder_conved, encoder_combined) # [batch size, y1 len - 1, output dim], attention_b = [batch size, y1 len - 1, x len]\n",
    "        \n",
    "        if (returns_attention):\n",
    "            return output_a, output_b, attention_a, attention_b\n",
    "        else:\n",
    "            return output_a, output_b\n",
    "\n",
    "    def getName(self):\n",
    "        return self.name  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model train and evaluate routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gc_count = 5\n",
    "\n",
    "def train_single_cnn(model,  dataset, optimizer, loss_fn, src_pad_idx, tgt_pad_idx, device, clip=None, smoothing = True):\n",
    "    model.train()       \n",
    "    epoch_loss = 0\n",
    "    epoch_accm = 0 \n",
    "    epoch_wmem = 0\n",
    "    epoch_lmem = 0   \n",
    "\n",
    "    i = 0\n",
    "    batches_before_gc = 0\n",
    "    for  src, tgt in iter(dataset):\n",
    "        \n",
    "        src = src.permute(1, 0)\n",
    "        tgt = tgt.permute(1, 0)\n",
    "        \n",
    "        tgt_input = tgt[:, :-1]\n",
    "\n",
    "        epoch_wmem += getSizeOf(src)\n",
    "        epoch_wmem += getSizeOf(tgt)\n",
    "\n",
    "        optimizer.zero_grad()                    \n",
    "\n",
    "        logits, _ = model(src, tgt_input)\n",
    "\n",
    "        epoch_lmem += getSizeOf(logits)\n",
    "\n",
    "        tgt_output = tgt[:,1:].contiguous().view(-1)\n",
    "        log_output = logits.contiguous().view(-1, logits.shape[-1])\n",
    "\n",
    "        loss = loss_fn(log_output, tgt_output)\n",
    "                    \n",
    "        loss.backward()\n",
    "\n",
    "        if (not clip == None):\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step() \n",
    "            \n",
    "        accm = masked_accuracy(tgt_output, log_output,  PAD_ES_IDX)\n",
    "\n",
    "        epoch_accm += accm\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        if i % 1000 == 999: \n",
    "            print(' - train - batch {0} loss: {1} acc: {2}'.format(i + 1, epoch_loss.item() / (i + 1), epoch_accm.item() / (i + 1) ))\n",
    "\n",
    "        epoch_wmem += getSizeOf(loss)\n",
    "        epoch_wmem += getSizeOf(accm)\n",
    "        i += 1\n",
    "    \n",
    "        del src\n",
    "        del tgt\n",
    "        del tgt_input\n",
    "        del tgt_output\n",
    "        del loss\n",
    "        del accm\n",
    "        del logits\n",
    "        batches_before_gc += 1\n",
    "\n",
    "        if (batches_before_gc > gc_count):\n",
    "            torch.cuda.empty_cache()       \n",
    "            batches_before_gc = 0\n",
    "\n",
    "    train_loss = (epoch_loss / batches_for_training).item()\n",
    "    train_accm = (epoch_accm / batches_for_training).item() \n",
    "    wrk_mem = epoch_wmem / batches_for_training\n",
    "    log_mem = epoch_lmem / batches_for_training    \n",
    "    return train_loss, train_accm, wrk_mem, log_mem\n",
    "    \n",
    "def eval_single_cnn(model,  dataset, loss_fn, src_pad_idx, tgt_pad_idx, device):\n",
    "    model.eval()    \n",
    "    epoch_loss = 0\n",
    "    epoch_accm = 0 \n",
    "    epoch_wmem = 0\n",
    "    epoch_lmem = 0  \n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        batches_before_gc = 0\n",
    "        for  src, tgt in iter(dataset):\n",
    "\n",
    "            src = src.permute(1, 0)\n",
    "            tgt = tgt.permute(1, 0)\n",
    "            \n",
    "            tgt_input = tgt[:, :-1]\n",
    "\n",
    "            epoch_wmem += getSizeOf(src)\n",
    "            epoch_wmem += getSizeOf(tgt)\n",
    "\n",
    "            logits, _ = model(src, tgt_input)\n",
    "\n",
    "            epoch_lmem += getSizeOf(logits)\n",
    "\n",
    "            tgt_output = tgt[:,1:].contiguous().view(-1)\n",
    "            log_output = logits.contiguous().view(-1, logits.shape[-1])\n",
    "\n",
    "            loss = loss_fn(log_output, tgt_output)\n",
    "\n",
    "            epoch_loss += loss\n",
    "            \n",
    "            accm = masked_accuracy(tgt_output, log_output,  PAD_ES_IDX)\n",
    "            \n",
    "            epoch_accm += accm\n",
    "          \n",
    "            if i % 1000 == 999: \n",
    "                print(' - eval - batch {0} loss: {1} acc: {2}'.format(i + 1, epoch_loss.item() / (i + 1), epoch_accm.item() / (i + 1) ))\n",
    "\n",
    "            epoch_wmem += getSizeOf(loss)\n",
    "            epoch_wmem += getSizeOf(accm)\n",
    "            i += 1\n",
    "\n",
    "            del src\n",
    "            del tgt\n",
    "            del tgt_input\n",
    "            del tgt_output\n",
    "            del logits\n",
    "            del loss\n",
    "            del accm\n",
    "            batches_before_gc += 1\n",
    "            if (batches_before_gc > gc_count):\n",
    "                torch.cuda.empty_cache()       \n",
    "                batches_before_gc = 0\n",
    "    \n",
    "    valid_loss = (epoch_loss / batches_for_validation).item()\n",
    "    valid_accm = (epoch_accm / batches_for_validation).item()\n",
    "    wrk_mem = epoch_wmem / batches_for_validation\n",
    "    log_mem = epoch_lmem / batches_for_validation\n",
    "    return valid_loss , valid_accm, wrk_mem, log_mem\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile, pstats, io\n",
    "from pstats import SortKey\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def run_single_cnn(model, filename, trnset, valset, optimizer, scheduler, loss_fn, src_pad_idx, tgt_pad_idx):\n",
    "    \n",
    "    \n",
    "    print(model.getName())    \n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    time_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print('[{0}] - Training / Testing Process - Started!'.format(time_str))\n",
    "\n",
    "    metrics_by_epoch = {}\n",
    "    \n",
    "    best_valid_loss = float(\"inf\")\n",
    "    \n",
    "    early_stopping_count = 0\n",
    "\n",
    "    base_epoch = -1 \n",
    "    # Check for previos training checkpoints (latest)\n",
    "    for epoch in range(NUMBER_OF_EPOCHS):\n",
    "        if os.path.exists(filename.format(epoch)):\n",
    "            base_epoch = epoch\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Load the latest checkpoint \n",
    "    if (base_epoch >= 0):\n",
    "        # Load model checkpoint file (latest)\n",
    "        checkpoint = torch.load(filename.format(base_epoch))\n",
    "        # Load model data only when match filename with epoch information\n",
    "        if (checkpoint['epoch'] == base_epoch):\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            metrics_by_epoch = checkpoint['metrics_by_epoch']\n",
    "            best_valid_loss = checkpoint['best_valid_loss']\n",
    "            early_stopping_count = checkpoint['early_stopping_count']\n",
    "            print(\"Continuing training model {0} from checkpoint at epoch {1}\".format(model.getName(), base_epoch + 1))\n",
    "            \n",
    "            for i in range(1, base_epoch):\n",
    "                print(\"- Epoch {0} achieves T.Loss = {1} E.Loss = {2}\".format(i, metrics_by_epoch[i]['train_loss'], metrics_by_epoch[i]['valid_loss']))\n",
    "            \n",
    "            print(\"- best_valid_loss = {0}\".format(best_valid_loss))\n",
    "        else:\n",
    "            raise RuntimeError(\"Checkpoint {0} is corrupt\".format(base_epoch))\n",
    "\n",
    "    if (base_epoch + 1 < NUMBER_OF_EPOCHS):\n",
    "        for epoch in range(base_epoch + 1, NUMBER_OF_EPOCHS):\n",
    "\n",
    "            trn_s_time = timer()       \n",
    "            train_loss, train_acc, trn_wrk_mem, trn_log_mem = train_single_cnn(model, trnset, optimizer, loss_fn,  src_pad_idx, tgt_pad_idx, DEVICE, clip=CLIPPING_VALUE)\n",
    "            trn_e_time = timer()\n",
    "            trn_elapsed_time = trn_e_time - trn_s_time\n",
    "\n",
    "\n",
    "            val_s_time = timer()  \n",
    "            valid_loss, valid_acc, val_wrk_mem, val_log_mem = eval_single_cnn(model, valset, loss_fn, src_pad_idx, tgt_pad_idx, DEVICE)\n",
    "            val_e_time = timer()\n",
    "            val_elapsed_time = val_e_time - val_s_time\n",
    "\n",
    "            scheduler.step(valid_loss)\n",
    "\n",
    "\n",
    "            print(\"Model {0} at Epoch {1} Duration = {2} second(s) LR = {3}\".format(model.getName(), epoch + 1, trn_elapsed_time + val_elapsed_time, scheduler.get_last_lr()))\n",
    "            print(\"Model {0} at Epoch {1} Trn.Loss = {2} and Trn.Accuracy = {3}\".format(model.getName(), epoch + 1, train_loss, train_acc))\n",
    "            print(\"Model {0} at Epoch {1} Val.Loss = {2} and Val.Accuracy = {3}\".format(model.getName(), epoch + 1, valid_loss, valid_acc))\n",
    "\n",
    "            metrics_by_epoch[epoch + 1] = dict(\n",
    "                model_name = model.getName(),\n",
    "                model_epoch = epoch + 1,\n",
    "                train_loss = train_loss,\n",
    "                train_acc = train_acc,\n",
    "                train_ppl = np.exp(train_loss),\n",
    "                train_wrkmem = trn_wrk_mem,\n",
    "                train_logmem = trn_log_mem,\n",
    "                valid_loss = valid_loss,\n",
    "                valid_acc = valid_acc,\n",
    "                valid_ppl = np.exp(valid_loss),\n",
    "                valid_wrkmem = val_wrk_mem,\n",
    "                valid_logmem = val_log_mem,\n",
    "                scheduler_lr = scheduler.get_last_lr(),\n",
    "                trn_elapsed_time = trn_elapsed_time,\n",
    "                val_elapsed_time = val_elapsed_time\n",
    "            )\n",
    "\n",
    "\n",
    "            temp_filename = \"temp_\" + filename.format(epoch)\n",
    "\n",
    "            torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'early_stopping_count': early_stopping_count,\n",
    "            'metrics_by_epoch' : metrics_by_epoch,\n",
    "            'best_valid_loss' : best_valid_loss\n",
    "            }, temp_filename)\n",
    "\n",
    "            os.rename(temp_filename, filename.format(epoch))\n",
    "\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                early_stopping_count = 0\n",
    "            elif epoch > EARLY_STOPPING_EPOCHS:\n",
    "                early_stopping_count += 1\n",
    "                      \n",
    "            if early_stopping_count == EARLY_STOPPING_EPOCHS:\n",
    "                print(\"Early stopping triggered in epoch {0}\".format(epoch + 1))\n",
    "                break\n",
    "        \n",
    "            tqdm.write(\"Waiting for gpu cooling time...\")\n",
    "\n",
    "            let_gpu_rest(INNER_GPU_REST_TIME)\n",
    "\n",
    "    return metrics_by_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CNN_ES_enc = GatedConvEncoder(INPUT_ES_VOCAB_DIM, GCNN_EMBDIM, GCNN_HID_DIM, GCNN_ENC_LAYERS, GCNN_ENC_KERNEL_SIZE, GCNN_ENC_DROPOUT, DEVICE).to(DEVICE)\n",
    "CNN_ES_dec = GatedConvDecoder(OUTPUT_ES_VOCAB_DIM, GCNN_EMBDIM, GCNN_HID_DIM, GCNN_DEC_LAYERS, GCNN_DEC_KERNEL_SIZE, GCNN_DEC_DROPOUT, CNN_TRG_PAD_IDX, DEVICE).to(DEVICE)\n",
    "\n",
    "CNN_FR_enc = GatedConvEncoder(INPUT_ES_VOCAB_DIM, GCNN_EMBDIM, GCNN_HID_DIM, GCNN_ENC_LAYERS, GCNN_ENC_KERNEL_SIZE, GCNN_ENC_DROPOUT, DEVICE).to(DEVICE)\n",
    "CNN_FR_dec = GatedConvDecoder(OUTPUT_FR_VOCAB_DIM, GCNN_EMBDIM, GCNN_HID_DIM, GCNN_DEC_LAYERS, GCNN_DEC_KERNEL_SIZE, GCNN_DEC_DROPOUT, CNN_TRG_PAD_IDX, DEVICE).to(DEVICE)\n",
    "\n",
    "CNN_model_EN_ES = GatedConvSeq2Seq(\"CNN_EN_ES\", CNN_ES_enc, CNN_ES_dec).to(DEVICE)\n",
    "CNN_model_EN_FR = GatedConvSeq2Seq(\"CNN_EN_FR\", CNN_FR_enc, CNN_FR_dec).to(DEVICE)\n",
    "\n",
    "CNN_optimizer_EN_ES = optim.Adam(CNN_model_EN_ES.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "CNN_optimizer_EN_FR = optim.Adam(CNN_model_EN_FR.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "scheduler_EN_ES = torch.optim.lr_scheduler.ReduceLROnPlateau(CNN_optimizer_EN_ES, 'min')\n",
    "scheduler_EN_FR = torch.optim.lr_scheduler.ReduceLROnPlateau(CNN_optimizer_EN_FR, 'min')\n",
    "\n",
    "CNN_Lossfn_EN_ES = nn.CrossEntropyLoss(ignore_index = CNN_TRG_PAD_IDX)\n",
    "CNN_Lossfn_EN_FR = nn.CrossEntropyLoss(ignore_index = CNN_TRG_PAD_IDX)\n",
    "\n",
    "CNN_DUAL_enc = GatedConvEncoder(INPUT_ES_VOCAB_DIM, GCNN_EMBDIM, GCNN_HID_DIM, GCNN_ENC_LAYERS, GCNN_ENC_KERNEL_SIZE, GCNN_ENC_DROPOUT, DEVICE).to(DEVICE)\n",
    "CNN_DUAL_ES_dec = GatedConvDecoder(OUTPUT_ES_VOCAB_DIM, GCNN_EMBDIM, GCNN_HID_DIM, GCNN_DEC_LAYERS, GCNN_DEC_KERNEL_SIZE, GCNN_DEC_DROPOUT, CNN_TRG_PAD_IDX, DEVICE).to(DEVICE)\n",
    "CNN_DUAL_FR_dec = GatedConvDecoder(OUTPUT_FR_VOCAB_DIM, GCNN_EMBDIM, GCNN_HID_DIM, GCNN_DEC_LAYERS, GCNN_DEC_KERNEL_SIZE, GCNN_DEC_DROPOUT, CNN_TRG_PAD_IDX, DEVICE).to(DEVICE)\n",
    "CNN_DUAL_model = GatedConvDualTaskSeq2Seq(\"CNN_EN_ES_FR\", CNN_DUAL_enc, CNN_DUAL_ES_dec, CNN_DUAL_FR_dec).to(DEVICE)\n",
    "CNN_DUAL_optimizer = optim.Adam(CNN_DUAL_model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "CNN_DUAL_Lossfn_ES = nn.CrossEntropyLoss(ignore_index = CNN_TRG_PAD_IDX)\n",
    "CNN_DUAL_Lossfn_FR = nn.CrossEntropyLoss(ignore_index = CNN_TRG_PAD_IDX)\n",
    "\n",
    "scheduler_DUAL = torch.optim.lr_scheduler.ReduceLROnPlateau(CNN_DUAL_optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "CNN_model_EN_ES.apply(init_transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_params(CNN_model_EN_ES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "CNN_model_EN_FR.apply(init_transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_params(CNN_model_EN_FR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "CNN_DUAL_model.apply(init_transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_params(CNN_DUAL_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "\n",
    "profile = True\n",
    "\n",
    "histories = []\n",
    "\n",
    "random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "CNN_model_EN_ES = CNN_model_EN_ES.to(DEVICE)\n",
    "\n",
    "\n",
    "history = None\n",
    "\n",
    "if (profile):\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "        with record_function(\"model_inference\"):\n",
    "            history = run_single_cnn(CNN_model_EN_ES, CNN_EN_ES_model_path, Bilingual_EN_to_ES_trnset, Bilingual_EN_to_ES_valset, CNN_optimizer_EN_ES, scheduler_EN_ES, CNN_Lossfn_EN_ES, PAD_EN_IDX, PAD_ES_IDX)\n",
    "\n",
    "        print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "        print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "else:\n",
    "    history = run_single_cnn(CNN_model_EN_ES, CNN_EN_ES_model_path, Bilingual_EN_to_ES_trnset, Bilingual_EN_to_ES_valset, CNN_optimizer_EN_ES, scheduler_EN_ES, CNN_Lossfn_EN_ES, PAD_EN_IDX, PAD_ES_IDX)\n",
    "\n",
    "\n",
    "name = CNN_model_EN_ES.getName()\n",
    "df = pd.DataFrame.from_dict(history, orient='index')   \n",
    "plot_accuracy_curve(name, df)\n",
    "plot_loss_curve(name, df)\n",
    "plot_pplx_curve(name, df)\n",
    "histories.append(df)\n",
    "torch.cuda.empty_cache()\n",
    "let_gpu_rest(OUTER_GPU_REST_TIME)\n",
    "\n",
    "\n",
    "random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "CNN_model_EN_FR = CNN_model_EN_FR.to(DEVICE)\n",
    "\n",
    "history = None\n",
    "\n",
    "if (profile):\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "        with record_function(\"model_inference\"):\n",
    "            history = run_single_cnn(CNN_model_EN_FR, CNN_EN_FR_model_path, Bilingual_EN_to_FR_trnset, Bilingual_EN_to_FR_valset, CNN_optimizer_EN_FR, scheduler_EN_FR, CNN_Lossfn_EN_FR, PAD_EN_IDX, PAD_FR_IDX)\n",
    "        print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "        print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "else:\n",
    "    history = run_single_cnn(CNN_model_EN_FR, CNN_EN_FR_model_path, Bilingual_EN_to_FR_trnset, Bilingual_EN_to_FR_valset, CNN_optimizer_EN_FR, scheduler_EN_FR, CNN_Lossfn_EN_FR, PAD_EN_IDX, PAD_FR_IDX)\n",
    "\n",
    "name = CNN_model_EN_FR.getName()\n",
    "df = pd.DataFrame.from_dict(history, orient='index')   \n",
    "plot_accuracy_curve(name, df)\n",
    "plot_loss_curve(name, df)\n",
    "plot_pplx_curve(name, df)\n",
    "histories.append(df)\n",
    "torch.cuda.empty_cache()\n",
    "let_gpu_rest(OUTER_GPU_REST_TIME)\n",
    "\n",
    "\n",
    "result = pd.concat(histories)\n",
    "result.to_csv(\"./metrics_gcnn_models.csv\", sep=';', index=True, encoding='utf-8')\n",
    "\n",
    "display(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
